---
title: "Computer Assignment 4"
author:
  - "Ville Sebastian Olsson (19911231-1999)"
  - "Arvind Guruprasad (19990212-6938)"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: true
  pdf_document:
    fig_caption: true
---

```{r, echo=FALSE, results='hide', fig.show='hide',message=FALSE}
suppressMessages(library(pROC))
source("assignment4.R")
list2env(ass, envir = .GlobalEnv)
knitr::opts_chunk$set(echo = FALSE, comment = "")
```

# Exercise 4:1 - Multiple Logistic Regression

In this exercise, we will build a logistic regression model to predict the survival of a patient admitted to the ICU. In this process, we also want to identify the significant risk factors that affect the probability of the survival outcome.

The data is provided as a CSV file that consists of records on 200 patients admitted to an Intensive Care Unit (ICU) during a certain period. It contains 21 columns in total with 1 unique identifier, 3 continuous variables, 15 binary variables,
and 2 nominal variables.
To do this, we will pre-process the data and build logistic regression models. We will then assess the goodness-of-fit to compare different logistic regression models.

## 1. Model selection

Our first objective is to select a model that strikes the right balance
between goodness-of-fit and model complexity.

### Data Pre-processing

Before feeding the data into a model, we investigate if
it is appropriate to first transform the data in some manner.

#### Collapsing categories

The dataset includes nominal variables for Ethnicity (`v5`) and Consciousness level (`v21`).

**Ethnicity**: We noticed that the level counts for `2` ('black') and `3` ('other') in Ethnicity (`v5`) were quite low relative to the level count for `1` ('white').
The counts for each level of \(V_5 \in \{1,2,3\}\) were:
```{r}
table(data_original$v5)
```
The count for the lowest level accounts for only \(10/(175+15+10)=5\%\)
of the total count, which is a small fraction.
Logistic regression requires sufficient data for each level of a categorical
variable to estimate the corresponding coefficients reliably.
With very small counts for levels `2` and `3`, the model may not be able to
accurately estimate the effect of these levels on the outcome.

To balance the distribution, levels `2` and `3` are combined into a single level (`0`: 'other'), with \(15+10 = 25\) subjects:
```{r}
table(data$v5)
```

**Consciousness level**: Similarly, the level count for `1` ('unconscious') and `2` ('coma') in Consciousness level (`v21`) are low compared to `0` ('awake').
```{r}
table(data_original$v21)
```

Hence, we combine levels `1` and `2` into a single level (`1`: unconscious and coma):
```{r}
table(data$v21)
```

#### Converting categorical variables to factors

We convert the relevant columns to factors to ensure they are treated as categorical variables in the logistic regression models. All the columns that do not contain numerical values are converted to factors here.

#### Non-predictive variables

Before training the models, it is recommended to discard the columns
in the data
that would not add any contextual significance in describing patient mortality.
In this case, all variables seem to be relevant in predicting patient mortality, with one exception: patient ID (`v1`).
This column contains unique identifiers for the patients and should, in principle, not provide any predictive value.
If there were to be an association between patient ID and survival outcomes, it would suggest that
the patient IDs were assigned in a non-random manner.
For example, perhaps patients who are treated at facility A are assigned a low patient ID, whereas
patients treated at facility B are assigned a high patient ID.
Including the patient ID would then mean that the model is trained to attribute
predictive power to the
patient ID, even though it is not inherently related to mortality.
For this reason, we discard this column before fitting the models.


### Model Selection

We start with an empty model that includes only the intercept. We then fit a full model that includes the main effects of all predictor variables. Using these two models as the lower and the upper bounds, we perform **stepwise model selection** using both AIC and BIC to compare the two models identified by the algorithm.

Since there are many predictor variables, there are many ways to combine them
into interactions. For the sake of minimizing model complexity, we will disregard
models that contain interactions between the variables.

#### Akaike's Information Criteria (AIC)

The Akaike Information Criterion (AIC) is a statistical measure used for model selection. It evaluates the quality of a statistical model by balancing the trade-off between goodness of fit (how well the model fits the data) and model complexity (the number of parameters in the model). AIC helps in identifying the model that explains the data with the minimum necessary complexity. The Akaike Information Criterion (AIC) is given by:

\[
AIC = -2 \cdot \ln(l) + 2k
\]

where:

- \(l\) = maximized value of the likelihood function of the model,
- \(k\) = number of parameters in the model.

The summary of the final model as a result of the step function using AIC:
```{r}
summary(mstep1_AIC)
```

#### Bayesian Information Criterion (BIC)

BIC is an alternative model selection criterion for evaluating the goodness
of fit of a model while penalizing model complexity to avoid overfitting.
It is similar in concept with that of AIC but the difference lies in its
penalty criteria. BIC penalizes complexity more heavily than AIC, especially
for large sample sizes. The Bayesian Information Criterion (BIC) is given by:

\[
BIC = -2 \cdot \ln(l) + k \cdot \ln(n)
\]

where:

- \(l\) = maximized value of the likelihood function of the model,
- \(k\) = number of parameters in the model,
- \(n\) = number of observations.

The summary of the final model as a result of the step function using BIC:
```{r}
summary(mstep1_BIC)
```

#### Comparison of the Model

AIC and BIC value of the AIC-optimized model:
```{r}
cat("AIC:", AIC(mstep1_AIC))
cat("BIC:", BIC(mstep1_AIC))
```

AIC and BIC value of the BIC-optimized model:
```{r}
cat("AIC:", AIC(mstep1_BIC))
cat("BIC:", BIC(mstep1_BIC))
```

Both models offer comparable AIC and BIC.
The AIC-optimized model offers the lowest AIC, prioritizing prediction accuracy,
while the BIC-optimized model provides the lowest BIC value, prioritizing model
simplicity given the shortage of predictor variables. Given the small sample
size of the data, the trade-off between complexity and accuracy of the
prediction is not significant. On prioritizing prediction accuracy over
simplicity, we will be choosing the AIC-optimized model. However, we will
continue to compare the BIC-optimized model in the upcoming sections on ROC
curves to ensure the other tests validate our choice.

#### Factors affecting patient survival

Finally, based on the chosen model, the factors that affect the probability of the patient admitted to the ICU to not survive include:

- **Consciousness Level (v21: Unconscious/Coma vs. Awake)**
- **Type of Admission (v14: Acute vs. Non-Acute)**
- **Age (v3)**
- **Cancer (v7: Yes vs. No)**
- **Blood Pressure (v11: Per mm Hg)**
- **Blood Carbon Dioxide (v18: Above 45 vs. Below 45)**
- **Blood pH (v17: Below 7.25 vs. Above 7.25)**


### Odds Ratios and Confidence Intervals

We calculate the odds ratios and confidence intervals for the coefficients of the final model.

```{r}
# Fit the final model
final_model <- glm(v2 ~ v21 + v14 + v3 + v7 + v11 + v18 + v17, family = binomial, data = data)

# Compute Odds and CI for final model's coefficients
exp_coef <- exp(coef(final_model))
conf_intervals <- exp(suppressMessages(confint(final_model)))
odds_ratios <- data.frame(
  Variable = names(exp_coef),
  Odds_Ratio = exp_coef,
  Lower_CI = conf_intervals[, 1],
  Upper_CI = conf_intervals[, 2]
)
print(odds_ratios)
```


#### Interpreting Coefficients

**Consciousness Level (v211: Unconscious/Coma vs. Awake)**

- OR: 89.68
- CI: (15.42, 894.72)

Patients who are unconscious or in a coma have **89.68 times higher odds** of not surviving compared to those who are awake. The large odds ratio reflects the severe impact of a lack of consciousness on survival outcomes.

---

**Type of Admission (v141: Acute vs. Non-Acute)**

- OR: 20.65
- CI: (4.03, 186.17)

Patients admitted for acute conditions have **20.65 times higher odds** of not surviving compared to non-acute admissions. Acute admissions likely indicate critical conditions requiring immediate attention, which correlates with poorer outcomes.

---

**Age (v3)**

- OR: 1.0436
- CI: (1.0179, 1.0746)

For every additional year of age, the odds of not surviving increase by **4.36%**.
Age is a significant factor in survival, likely due to declining health and resilience in older patients.

---

**Cancer (v71: Yes vs. No)**

- OR: 11.05
- CI: (1.99, 73.21)

Patients with cancer have **11.05 times higher odds** of not surviving compared to those without cancer.
The presence of cancer reflects a chronic condition that exacerbates the risk of mortality.

---

**Blood Pressure (v11: Per mm Hg)**

- OR: 0.986
- CI: (0.971, 0.999)

For each 1 mm Hg increase in blood pressure, the odds of not surviving decrease by **1.44%**.
A higher blood pressure within a safe range might reflect better cardiovascular health, reducing mortality risk. However, the difference is small.
It should also be noted that the result is barely significant, as the confidence
interval almost includes 1.

---

**Blood Carbon Dioxide (v181: Above 45 vs. Below 45)**

- OR: 0.1051
- CI: (0.012, 0.641)

Patients with carbon dioxide levels above 45 have significantly **lower odds** of not surviving (89.49% decrease) compared to those with levels below 45.
Elevated carbon dioxide may reflect physiological compensation mechanisms for certain conditions.

---

**Blood pH (v171: Below 7.25 vs. Above 7.25)**

- OR: 6.39
- CI: (1.14, 38.48)

Patients with a blood pH below 7.25 have **6.39 times higher odds** of not surviving compared to those above 7.25.
The magnitude of the odds ratio is quite large, implying that low blood pH is a clinically important factor associated with survival.

---

**Intercept**

- Odds Ratio (OR): 0.0049
- Confidence Interval (CI): (0.00016, 0.0961)

The baseline odds of not surviving when all predictors are at their reference levels are extremely low.
Under baseline conditions, the odds of not surviving are extremely small compared to the odds of surviving, at least within this study population.

---

Below is the summary of the various factors used in the final model distinguished based on its significance:

1. **Significant Risk Factors for Not Surviving:**

   - Consciousness Level (Unconscious/Coma)
   - Type of Admission (Acute)
   - Age
   - Cancer
   - Blood pH (Below 7.25)

2. **Protective Factors:**
   - Blood Pressure (Higher)
   - Blood Carbon Dioxide (Above 45)

Protective predictors refer to variables that are associated with a decreased risk of the outcome of interest (in this case, not surviving) when their values increase or change in a specific way. Essentially, these predictors reduce the likelihood of the patient not surviving. Whereas Significant risk factors are the opposite. They increase the likelihood of the patient not surviving with increase in their values.

These results underscore the importance of critical physiological and clinical indicators in predicting survival outcomes in ICU patients.

## 2. Goodness of Fit

Now that we have chosen the model, we need to understand how well the chosen
model fits the data. We do this using the Hosmer-Lemeshow goodness-of-fit test.
This test is widely used to evaluate the
calibration of logistic regression models, particularly when the data is at the individual level.
Unlike deviance-based tests, which are suitable for grouped data, this test measures how well the predicted probabilities from the model align with the observed outcomes across subgroups of the data.

The test statistic for the Hosmer-Lemeshow test is given by:

\[
\chi^2 = \sum_{g=1}^{G} \frac{(O_g - E_g)^2}{E_g (1 - \frac{E_g}{N_g})}
\]

Where:

- \( g \): Index for each group (e.g., deciles of risk).
- \( O_g \): Observed count of events (e.g., non-survivals) in group \( g \).
- \( E_g \): Expected count of events in group \( g \) based on the model.
- \( N_g \): Total number of observations in group \( g \).
- \( G \): Total number of groups (typically 10).

The test statistic follows a chi-squared distribution with \( G - 2 \) degrees of freedom.

We follow the common convention of running the test with \(G=10\) groups.
Upon running this test, we obtain:

```{r}
print(hoslem_test)
```

The p-value of \(0.5131\) is greater than the significance level of 0.05.
This indicates that there is no statistically significant evidence to reject
the null hypothesis of the test. The null hypothesis assumes that the model fits
the data well. Hence, the test suggests that the logistic regression model
provides a good fit to the observed data.

The \(\chi^2\) value of \(7.2202\) is relatively small considered that there
are 8 degrees of freedom, indicating a minor deviation between the observed
and expected frequencies across the groups.

Since the result of the test is not statistically significant,
we have found no evidence that
the logistic regression model
is poorly calibrated or that it fits the data poorly.

## 3. Confusion Matrices and Metrics

This section involves evaluating the predication performance of the chosen
logistic regression model using a confusion matrix.
The metrics used for evaluation are:

1. Accuracy: Overall correctness of the model.

   \[
   \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{Total}}
   \]
   where:

   - TP = True Positives
   - TN = True Negatives

2. Sensitivity (True Positive Rate): Ability to identify "Not Survive" cases correctly.

   \[
   \text{Sensitivity} = \frac{\text{TP}}{\text{TP} + \text{FN}}
   \]
   where:

   - TP = True Positives
   - FN = False Negatives

3. Specificity (True Negative Rate): Ability to identify "Survive" cases correctly.
   \[
   \text{Specificity} = \frac{\text{TN}}{\text{TN} + \text{FP}}
   \]
   where:

   - TN = True Negatives
   - FP = False Positives

We first define a set of suitable cutoff values.
For each cutoff value,
we compare the predicted probabilities against the cutoff value and assign a binary
outcome to determine the patient mortality.
For each cutoff value \(c\) and predicted probability \(p\),
we assign the outcome as "Not Survive"
(`1`) if \(p > c\) and "Survive" (`0`) otherwise.
We then compare this predicted
binary outcome against the actual outcome provided in the original dataset.
Based on this comparison, we create a table (confusion matrix) consisting of
the true positives, true negatives, false positives, and false negatives.
This will allow us to numerically compute accuracy, sensitivity, and
specificity.

To gauge how the predicted probabilities are varying with the actual outcome,
we evaluate the model performance for the following cutoff values:
\[c \in \{0.5, 0.75, 0.25\}\]

Result:
```{r}
predicted_probs <- predict(final_model, type = "response")
for (cutoff in cutoffs) {
  # Convert probabilities to binary predictions based on the cutoff
  predicted_class <- ifelse(predicted_probs > cutoff, 1, 0)

  # Generate the confusion matrix
  confusion_matrix <- table(Actual = data$v2, Predicted = predicted_class)

  # Calculate metrics
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  sensitivity <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ]) # TP / (TP + FN)
  specificity <- confusion_matrix[1, 1] / sum(confusion_matrix[1, ]) # TN / (TN + FP)

  # Store the results
  results[[as.character(cutoff)]] <- list(
    Confusion_Matrix = confusion_matrix,
    Accuracy = accuracy,
    Sensitivity = sensitivity,
    Specificity = specificity
  )
}
# Print metrics for each cutoff
for (cutoff in cutoffs) {
  cat("\nCutoff:", cutoff, "\n")
  print(results[[as.character(cutoff)]]$Confusion_Matrix)
  cat("Accuracy:", results[[as.character(cutoff)]]$Accuracy, "\n")
  cat("Sensitivity:", results[[as.character(cutoff)]]$Sensitivity, "\n")
  cat("Specificity:", results[[as.character(cutoff)]]$Specificity, "\n")
}
```

We can observe the following:

1. Cutoff = 0.5:
At this threshold, the model achieves high accuracy (0.87) and specificity (0.981), meaning it correctly identifies most of the "Survive" cases. However, sensitivity (0.425) is low, indicating the model struggles to identify "Not Survive" cases.

2. Cutoff = 0.75:
Increasing the cutoff reduces sensitivity (0.325) further (fewer "Not Survive" cases are identified) but increases specificity (0.993) slightly. This cutoff might prioritize avoiding false positives ("Not Survive" predicted when "Survive") over identifying all "Not Survive" cases. The accuracy (0.86) decreased slightly.

3. Cutoff = 0.25:
Lowering the cutoff significantly improves sensitivity (0.7) (better identification of "Not Survive" cases) but reduces specificity (0.85) and overall accuracy (0.82). This cutoff is useful when it is more important to correctly identify "Not Survive" cases.

For predicting survival in ICU, sensitivity is especially important as the cost of missing
critical cases is high compared to the cost of over-monitoring patients.
The model should ensure it captures the majority of high-risk patients, even if
it results in some false positives. However, this is completely subjective.
We could also balance sensitivity and specificity when
ICU resources (e.g., staff, equipment) are limited, and one needs to minimize
unnecessary interventions while still identifying most critical cases.

## 4. ROC Curves and AUC

From the previous section, we learned that it is desirable to choose an
optimal cutoff
value to prioritize sensitivity or balance sensitivity and specificity.
To more easily identify the ideal cutoff, we make use of ROC curves and AUC.

A ROC curve plots the true positive rate (sensitivity) against the false
positive rate (1 - specificity) at various threshold settings. ROC curves help
evaluate the performance of a binary classifier across different classification
thresholds. The ROC curve shows the trade-off between sensitivity and
specificity for a given model. The AUC is a single metric summarizing
the performance of a classifier, where a higher AUC indicates
better model performance.

We will study three models:

- The full model that captures all the main effects of the predictor variables available in the dataset,
- The AIC-optimized model we found earlier, and
- The BIC-optimized model we found earlier.

For each model, we plot a ROC curve and calculate the AUC:

```{r}
# Plot the ROC curves
plot(roc_full, col = "blue", legacy.axes = TRUE, main = "ROC Curves for Logistic Models")
lines(roc_model_1, col = "red") # Add Model 1 ROC curve
lines(roc_model_2, col = "green") # Add Model 2 ROC curve

# Add legend
legend("bottomright",
  legend = c("Full Model", "Model 1 (AIC-optimized)", "Model 2 (BIC-optimized)"),
  col = c("blue", "red", "green"), lwd = 2
)
```

AUC:
```{r}
cat("AUC for Full Model:", auc(roc_full), "\n")
cat("AUC for Model 1 (AIC-optimized):", auc(roc_model_1), "\n")
cat("AUC for Model 2 (BIC-optimized):", auc(roc_model_2), "\n")
```

#### Key Insights

Upon observing the plots, we notice a lot of overlap between the three curves,
indicating similar performance. Investigating further:

- The full model corresponds to the highest ROC curve and greatest AUC,
indicating the best performance among the three models.
The AUC (`0.8847`) indicates excellent performance, with high sensitivity
and specificity across thresholds.

- Model 1 performs slightly worse than the full model but better than model 2.
The AUC (`0.8678`) here is slightly lower than the full model,
but still a strong classifier.

- Model 2 has the lowest curve, indicating comparatively weaker classification
performance. Here, the AUC (`0.8533`) is the least performing compared to the
other models, though still not far behind the two others.

#### Conclusion

- The full model is the best performer, as evidenced by the highest AUC (`0.8847`) and the highest ROC curve.
- Model 1 offers a good trade-off, with reduced complexity and slightly lower AUC (`0.8678`).
- Model 2 sacrifices performance for simplicity, resulting in the lowest AUC (`0.8533`).

Based on the results above, a decision has to be made between
simpler interpretation and predictive performance.
If the emphasis is on simpler interpretation with minimal loss in performance,
model 1 will be considered.
However, the model with the highest AUC is the full model.

## 5. LOOCV-adjusted ROC Curves and AUC

In the previous section, we constructed the ROC curves and computed the AUC for each curve for the same dataset that was used for model fitting. This may result in overfitting and to combat this, we make use of the validation technique called LOOCV.

Leave-One-Out Cross-Validation (LOOCV) is a validation technique used to estimate the predictive performance of a model while minimizing overfitting. It is especially useful in small datasets, such as this ICU study, where holding out a significant portion of the data (e.g., using 80-20 split) might not leave enough data for training.

#### Key Advantages:
- **Avoids Overfitting**: By iteratively training the model on all data except one observation and then testing on the excluded observation, LOOCV ensures the model's performance is not overly optimistic.
- **Reliable Estimate**: Each data point serves as the validation set once, leading to an almost unbiased estimate of prediction error.

We will now implement LOOCV for the three models we selected earlier including the full model, Model 1 (AIC), and Model 2 (BIC).

To implement LOOCV, we follow the below steps:

1. Partitioning: The dataset with \(n\) observations is divided such that in each iteration, \(n-1\) observations are used for training, and the remaining 1 observation is used for validation.

2. Iterative Training and Prediction:
  - For each iteration \(i\) (from \(1\) to \(n\)):
    - Exclude the \(i\)-th observation from the training data.
    - Train the logistic regression model on the remaining \(n-1\) observations.
    - Predict the probability of the excluded observation using the trained model.

3. Performance Calculation:
  - After iterating through all \(n\) observations, predictions are collected.
  - The performance metric (e.g., ROC curve and AUC) is calculated using the actual outcomes and the LOOCV-predicted probabilities.

The LOOCV-adjusted AUC and ROC curves are shown below:

```{r}
cat("LOOCV AUC for Full Model:", loocv_full$AUC, "\n")
cat("LOOCV AUC for Model 1 (AIC):", loocv_model_1$AUC, "\n")
cat("LOOCV AUC for Model 2 (BIC):", loocv_model_2$AUC, "\n")

# Plot the ROC curves
plot(loocv_full$ROC, col = "blue", legacy.axes = TRUE, main = "LOOCV ROC Curves")
lines(loocv_model_1$ROC, col = "red")
lines(loocv_model_2$ROC, col = "green")

# Add a legend
legend("bottomright",
  legend = c("Full Model", "Model 1 (AIC)", "Model 2 (BIC)"),
  col = c("blue", "red", "green"), lwd = 2
)
```

The plot above shows the ROC curves for three logistic regression models:

1. **Full Model (Blue)**: Includes all predictors.
2. **Model 1 (Red)**: Includes predictors selected using AIC-based stepwise selection.
3. **Model 2 (Green)**: Includes predictors selected using BIC-based stepwise selection.

####  Interpretation

1. **Shape of the Curves**:
   - All three curves show the relationship between **Sensitivity (True Positive Rate)** and **1 - Specificity (False Positive Rate)** across different classification thresholds.
   - A curve closer to the top-left corner of the graph represents better performance.
   - Model 2 (BIC) and the Model 1 (AIC) indicate higher performance compared to the full model.

2. **Comparison of Models**:
   - **Model 2 (Green)** has the highest AUC (0.8244) and demonstrates slightly better performance than the other two models.
   - **Model 1 (Red)** has an AUC of 0.8203, which is very close to Model 2, indicating comparable performance.
   - **Full Model (Blue)** has the lowest AUC (0.7547), suggesting that including all predictors does not improve predictive performance, potentially due to overfitting or the inclusion of less relevant predictors. This suggests that the full model may be less effective at distinguishing between "Survive" and "Not Survive" outcomes.


#### Effects of LOOCV-adjustment

| Model           | AUC (Non-LOOCV) | AUC (LOOCV) | Difference |
|------------------|-----------------|-------------|------------|
| Full Model       | 0.8847          | 0.7547      | -0.1300    |
| Model 1 (AIC)    | 0.8678          | 0.8203      | -0.0475    |
| Model 2 (BIC)    | 0.8533          | 0.8244      | -0.0289    |


1. **Full Model**:
   - AUC drops significantly from **0.8847** to **0.7547** with LOOCV adjustment.
   - This indicates that the full model likely overfits the training data, as its performance on unseen data (approximated by LOOCV) is much worse.

2. **Model 1 (AIC)**:
   - AUC decreases modestly from **0.8678** to **0.8203**, showing a more balanced model with better generalization than the full model.
   - Stepwise selection using AIC reduces overfitting by focusing on significant predictors.

3. **Model 2 (BIC)**:
   - AUC decreases slightly from **0.8533** to **0.8244**, demonstrating the best generalization among all models.
   - BIC penalizes model complexity more heavily than AIC, which likely results in a simpler, more robust model.

**Effects Observed:**

1. Overfitting Detection:
   - The full model exhibits the largest drop in AUC (from **0.8847** to **0.7547**), indicating overfitting due to the inclusion of all predictors.

2. Preference for Simpler Models:
   - Both Model 1 (AIC) and Model 2 (BIC) show smaller reductions in AUC, highlighting their better ability to generalize.
   - Model 2 (BIC), which is simpler than Model 1 (AIC), maintains the highest LOOCV-adjusted AUC, making it the preferred model.

3. Model Selection:
   - Without LOOCV, the Full Model might appear superior due to its higher AUC on the training data.
   - With LOOCV, Model 2 emerges as the most robust choice, balancing performance and simplicity.

#### **Conclusion**

LOOCV highlights the importance of model simplicity by penalizing overfitting, making it an essential step in robust model selection. **Model 2 (BIC)** maintains the highest LOOCV-adjusted AUC, making it the preferred model.

# Exercise 4:2 - Decision Tree

## 1. Decision trees

We will now study the usefulness of decision trees
as an alternative to the logistic models.
We start by generating decision trees using both splitting methods
(Gini index and information gain)
with the following values of the complexity parameter:
\[\text{cp} \in \{0.1, 0.01, 0.001\}\]

We plot each pair of trees and study the structure of each tree below.

```{r}
plot_decision_trees(data4, 0.1)
```

For a high value of the complexity parameter (\(\text{cp}=0.1\)),
we see that the decision tree consists of only three nodes,
with a single split separating the top node from the leaves.
In this case, both splitting methods produced the same tree.

```{r}
plot_decision_trees(data4, 0.01)
```

For a medium value of the complexity parameter (\(\text{cp}=0.01\)),
we see that the decision tree now consists of five nodes and two splits.
Again, both splitting methods produced the same tree.
Perhaps the choice of splitting method does not matter much when
the requested trees are relatively simple.

```{r}
plot_decision_trees(data4, 0.001)
```

For a low value of the complexity parameter (\(\text{cp}=0.001\)),
we see that
the choice of splitting method affects the structure of the tree.
The Gini index tree consists of 11 nodes and 5 splits,
whereas the Information gain tree consists of 15 nodes and 7 splits.
It appears that the lower the value of the complexity parameter,
the more complex the tree.

### Interpretation

A good decision tree is one that strikes the right balance between
interpretability and predictive power.
A decision tree which has mediocre predictive power
can still be useful if the tree simple enough so that non-experts can
understand it.

Nevertheless,
let us examine the structure of the most complex decision
tree we found.
This is the tree that uses the information gain splitting method
with a complexity parameter value of \(\text{cp}=0.001\).
Although the tree is relatively complex, it may pay off in predictive power.
We will determine whether this is the case in the next section.

```{r}
rpart.plot(tm_good, main = paste(tm_good_split_name, "with cp =", tm_good_cp))
```

The tree consists of 15 nodes and 7 splits.
The tree predicts the target variable \(V_2\) (survival),
which has two possible outcomes, \(V_2=0\) (survived) and \(V_2=1\) (passed away).

* Top node: Assuming we do not know anything about the patient,
the probability of non-survival is 0.20.

* `v21=0`: This is the first split.
If the patient is conscious (\(V_{21}=0\)),
we proceed to the left child node.
If not, we proceed to the right child node,
which states that the patient will pass away with a probability
of 0.87. This is predicted to happen for 8 % of all patients.

* `v11>=88`: This is the second split. If the blood pressure is high (\(V_{11}\geq 88\) mm Hg),
we proceed to the left child node.
If not, the patient passes away with a probability of 0.67.
Only 4 \% of the patients suffer this fate.

* `v14=0`: This is the third split.
If the admission is of type "not acute" (\(V_{14}=0\)),
the patient is predicted to pass away with a probability of only 0.02.
If it is acute, we proceed to the right child node.

* `v3<49`: This is the fourth split.
If the patient is less than 49 years old (\(V_3\leq 49\)),
the probability of passing away is only 0.04.
If not, we proceed to the right child node.

* `v11>=149`: This is the fifth split.
If the blood pressure is very high (\(V_{11}\geq 149\) mm Hg),
the patient is predicted to pass away with a probability of
only 0.09. If not, we proceed to the right child node.

* `v3<74`: This is the sixth split.
If the patient is less than 74 years old (\(V_3<74\)),
we proceed to the left child node.
If not, the probability of passing away is 0.42.

* `v11<137`: This is the seventh and final split.
If the blood pressure is relatively low (\(V_{11}<137\) mm Hg),
the patient is predicted to pass away with a probability of
only 0.14. If not, the patient is predicted
to pass away with a probability of 0.57.

## 2. Predictive power

Next, let us evaluate the predictive power of our decision tree.
For comparison, we will plot the ROC curve for
both the decision tree and the logistic regression model
we chose at the end of Exercise 4:1.
This refers to the model selected based on the BIC criterion, with formula:
\[V_2 \sim V_{21} + V_{14} + V_3 + V_7\]

```{r}
plot_tree_against_logistic(tree_roc, logistic_roc4)
```

AUC for the decision tree:
```{r}
tree_roc$auc
```

AUC for the logistic model:
```{r}
logistic_roc4$auc
```

As we can see, the ROC curves for the logistic model
and the decision tree overlap for the most part,
suggesting that the two models are approximately equal
in predictive power.
The decision tree is even shown to outperform
the logistic model overall,
as evidenced by the higher AUC value.
In particular, if the specificity is approximately 0.8,
we see that the decision tree yields better values of sensitivity.
However, we should take these results with a grain of salt,
as this is done without any correction for overfitting.

## 3. LOOCV-correction

To correct for overfitting, we will again perform
leave-one-out-cross-validation and recompute
the ROC and AUC.

```{r}
plot_tree_against_logistic(tree_roc_loocv, logistic_roc_loocv4)
```

AUC for the decision tree:
```{r}
tree_roc_loocv$auc
```

AUC for the logistic model:
```{r}
logistic_roc_loocv4$auc
```

After applying LOOCV, we see that
the predictive power of both models dropped.
The AUC for the decision tree is now considerably lower than that of
the logistic model, implying that the decision tree is
comparatively worse overall.
At a specificity around 1.0, the two models
are shown to be equally matched.
Between specificity values of 0.6 and 1.0,
we see that the ROC curve of the logistic
model is above that of the decision tree,
suggesting that the logistic model outperforms
the decision tree within this range.
If high specificity is desired, the
decision tree will probably not be useful.

On the other hand, the decision tree is shown
to outperform the logistic model
for low values of specificity (0.0 to around 0.6).
So if we are willing to accept a high rate of
false positives, the decision tree may be a better choice.
Indeed, in the context of an ICU, it may
be wiser to err on the side of caution and
assume that the patient is dying in times of doubt,
as the decision we make is a matter of life or death.

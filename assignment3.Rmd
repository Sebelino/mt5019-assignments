---
title: "Computer Assignment 3"
author:
  - "Ville Sebastian Olsson (19911231-1999)"
  - "Arvind Guruprasad (19990212-6938)"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: true
  pdf_document:
    fig_caption: true
---

```{r, echo=FALSE, results='hide', fig.show='hide'}
source("assignment3.R")
list2env(ass, envir = .GlobalEnv)
knitr::opts_chunk$set(echo = FALSE, comment = "")
```

# Exercise 3:1

## 1. Model finding

The data comes from a study reported by Wermuth (1976), which was collected in a birth clinic. The dataset includes the following variables:

- **Mother's age (\(X\)):** `<30` and `30+`.
- **Smoking habits (\(Y\)):** `<5 cigarettes/day` and `5+ cigarettes/day`.
- **Gestational age (\(Z\)):** `<260 days` and `â‰¥260 days`.
- **Child survival (\(V\)):** Binary outcome indicating survival (`Yes`/`No`).

Our objective is to find a suitable loglinear model
which

**TODO: Is this approach correct?**

To generate a table, we follow this process:

1. Read the CSV data.
2. Fit the saturated model, \((XYZV)\).
3. For each combination of interaction terms, fit a model. Each combination must include the main effects, i.e. \(X\), \(Y\), \(Z\) and \(V\).
4. For each sub-model, calculate deviance, Akaike's information criterion (AIC) and p-value with respect to the saturated model.

The deviance is used to calculate the p-value as follows:
\[\text{p-value} = P(\chi^2_{\text{df}} \geq T_{\chi^2})\]
where \(T_{\chi^2} = 2(\ell(XYZ) - \ell(M))\) denotes the deviance and \(\text{df}\)
is the degrees of freedom. \(\ell(M)\) denotes the log-likelihood under model \(M\).

The degrees of freedom is calculated as:
\[\text{df} = IJKL - \text{number of parameters of }M\]
where \(IJKL = 2\cdot2\cdot2\cdot2 = 16\) is the number of cells in the contingency table.

The AIC of a model \(M\) is given by:
\[\text{AIC}(M) = -2\ell(M) + 2p\]
where \(p\) is the number of parameters in \(M\),
including intercept terms.

We can use the function `step` to find the model with the lowest AIC.
Since we are supposed to find this model by
starting with the saturated model (`x*y*z`) and then
removing (but not adding)
interaction terms, we must run `step` with `direction="backward"`.

Result:
```{r}
data <- read.csv("data_ca3.csv")
msat <- glm(n ~ x * y * z * v, family = poisson, data = data)
mstep_AIC<-step(msat, direction="backward", trace=TRUE, scope = list(upper = n~x*y*z*v, lower = n~x+y+z+v))
```

TODO: but higher interaction terms should be
removed before lower-interaction terms, and that's
not happening here.
The 2-way `y:v` term is removed before any of the remaining 3-way terms:
<pre><code>
n ~ x + y + z + v + x:y + x:z + y:z + x:v + z:v + y:v + x:y:v + x:z:v + y:z:v + x:y:z + x:y:z:v
n ~ x + y + z + v + x:y + x:z + y:z + x:v + z:v + y:v + x:y:v + x:z:v + y:z:v + x:y:z
n ~ x + y + z + v + x:y + x:z + y:z + x:v + z:v + y:v + x:y:v + x:z:v + y:z:v
n ~ x + y + z + v + x:y + x:z + y:z + x:v + z:v + <span style="background-color: yellow">y:v</span> + x:y:v + x:z:v
n ~ x + y + z + v + x:y + x:z + x:v + y:v + z:v + x:y:v + x:z:v
n ~ x + y + z + v + x:y + x:z + x:v + y:v + z:v + x:y:v
n ~ x + y + z + v + x:y + x:z + x:v + y:v + z:v
</code></pre>


Result:
```{r}
model_table
```
TODO: Ask about whether to list all combinations of interaction terms vs. total ordering of models, where set n is a subset of set n-1

## 2. Model selection

The table contains the following information which helps us
select the best model:

**1. Deviance**

- Measures the difference in goodness-of-fit between the submodel and the saturated model.
- A low deviance indicates a good fit.

**2. Degrees of Freedom**

- Reflects the number of parameters removed from the saturated model.
- Higher df means a simpler model.

**3. p-value**

- Tests whether the submodel is significantly worse than the saturated model.
- If \( p > 0.05 \), the simpler model is adequate.

**4. AIC**

- Evaluates the trade-off between model complexity and fit.
- Select the model with the lowest AIC.

```{r}
model_table[order(model_table$AIC),]
```

The table above has been sorted by AIC in ascending order,
to gauge which of the simpler model would be effective with
respect to the saturated model.

We should select the model that satisfies two criteria:

* Minimal AIC value
* The p-value is greater than 0.05

Based on the table above, we want to select model 6,
which includes the interaction terms `XV, YV, ZV, XZ, XY, YZ`.

# 3. Model interpretation

The model we selected looks like this:

```{r}
best_model
```

Given the lack of three-way interactions, this model
is much simpler relative to the relative model and the
other three-way interaction model while retaining
its ability to fit the data reasonably well.

```{r}
associations
```

\[OR_Y = \frac{odds(Y=1)}{odds(Y=0)}\]

Critical Factors:

- **The main effects of \( Y \) (smoking) and \( Z \) (gestational age)** are significant predictors of child survival.
- **The interaction \( V : Z \) (child survival and gestational age)** has the strongest effect, highlighting the importance of adequate gestational periods for child survival.

Non-significant Effects:

- The interactions \( V : Y \), \( X : Z \), and \( Y : Z \) are not statistically significant, suggesting these effects are not strong enough to influence survival outcomes in this dataset.

Implications:

- Reducing smoking among mothers and improving gestational age outcomes are likely to have the most significant positive impact on child survival rates.


# 4. Logistic model

```{r}
model_table4
```

Based on the table, AIC is minimized for the
main effects model \((X, Y, Z)\)
(with AIC: `1092.757`),
so we consider it the model that strikes the best balance
between goodness-of-fit and model complexity.

Next, we calculate the odds ratio and associated confidence intervals
and p-values of this model's coefficients:

```{r}
results4
```

The odds ratio for the smoking risk factor (`y5+`) indicates that mothers
who smoke 5+ cigarettes per day
have 65.5% of the odds of child survival compared to mothers who smoke less
than 5 cigarettes per day, holding all other variables constant.
Since the odds ratio is less than 1, it suggests a negative association between smoking heavily and child survival.

```{r}
data <- read.csv("data_ca3.csv")
msat <- glm(v ~ x * y * z, weights=n, family = binomial, data = data)
mstep_AIC<-step(msat, direction="backward", trace=TRUE, scope = list(upper = v~x*y*z, lower = v~x+y+z))
```

# 5. Logistic-loglinear model relationship

In problem 4, we selected the logistic regression model with the lowest AIC
value, namely:
\[V \sim X + Y + Z\]

There exists a corresponding loglinear model that
yields identical estimates and standard errors for the corresponding parameters.
The model must include a specific set of interaction terms.

There are \(m=4\) binary variables involved, namely \(X\), \(Y\), \(Z\), and \(V\). 
The loglinear model must contain the terms of the following types:

* A: The same terms as the logistic model, in this case \(X\), \(Y\), and \(Z\)
* B: All possible interactions of order \(m-1=3\) and lower between the three explanatory variables, in this case \(X:Y\), \(X:Z\), \(Y:Z\), and \(X:Y:Z\)
* C: The response variable \(V\)
* D: Interactions between the response variable \(V\) and each term in the logistic model,
i.e. \(X:V\), \(Y:V\), and \(Z:V\)

Hence, we end up with the following formula:
\[
N \sim
\underbrace{X + Y + Z}_{A} +
\underbrace{X:Y + X:Z + Y:Z + X:Y:Z}_{B} +
\underbrace{V}_{C} +
\underbrace{X:V + Y:V + Z:V}_{D}
\]

Upon fitting both models,
we should expect the coefficients listed in Table 1 to have equal values.

| Logistic model | Loglinear model |
|:--------------:|:---------------:|
| Intercept      | \(V\)           |
| \(X\)          | \(X:V\)         |
| \(Y\)          | \(Y:V\)         |
| \(Z\)          | \(Z:V\)         |
<div style="text-align: center;">
*Table 1: Each term in the logistic model and its matching term in the loglinear model.*
</div>

## Estimates

Fitting the logistic model yields the following estimates:

```{r}
data <- data.frame(
  x = c(0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1),
  y = c(1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1),
  z = c(0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1),
  v = c(1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1),
  n = c(40, 315, 9, 50, 6, 24, 459, 4012, 11, 4, 41, 147, 1594, 1, 14, 124)
)
model4 <- glm(v ~ x + y + z, weights=n, family = binomial(link=logit), data = data)
coefficients <- summary(model4)$coefficients
beta <- coefficients[, "Estimate"]
print(beta)
```
Fitting the equivalent loglinear model yields the following estimates:
```{r}
# https://stats.stackexchange.com/questions/476742/cant-find-loglinear-models-corresponding-logistic-regression-model
# https://teaching.sociology.ul.ie/SSS/lugano/node58.html
model4_loglinear <- glm(n ~ x + y + z + v
                        + x:y
                        + x:z
                        + x:v
                        + y:z
                        + y:v
                        + z:v
                        + x:y:z
                        ,
                        family = poisson(link=log), data = data)
coefficients_ll <- summary(model4_loglinear)$coefficients
beta_ll <- coefficients_ll[, "Estimate"]
print(beta_ll)
```
When comparing each pair of terms according to Table 1,
we see that each pair of coefficient values are virtually identical:
```{r}
comparison <- data.frame(
  LogisticParam = c("(Intercept)","x","y","z"),
  LoglinearParam=c("v","x:v","y:v","z:v"),
  Logistic = beta[c("(Intercept)", "x", "y", "z")],
  Loglinear = beta_ll[c("v", "x:v", "y:v", "z:v")],
  Difference = c(
    beta_ll["v"]-beta["(Intercept)"],
    beta_ll["x:v"]-beta["x"],
    beta_ll["y:v"]-beta["y"],
    beta_ll["z:v"]-beta["z"]
  ),
  row.names=NULL
)
print(comparison)
```
The biggest difference is smaller than \(10^{-10}\).
This difference is negligible and likely due to loss of numerical precision
when the values were computed.

## Standard errors
Calculating the standard errors yields similar results.

Standard errors in the logistic model:
```{r}
se <- coefficients[, "Std. Error"]
print(se)
```
Standard errors in the loglinear model:
```{r}
se_ll <- coefficients_ll[, "Std. Error"]
print(se_ll)
```
Again, we see that the standard errors are virtually identical:
```{r}
comparison_se <- data.frame(
  LogisticParam = c("(Intercept)","x","y","z"),
  LoglinearParam=c("v","x:v","y:v","z:v"),
  Logistic = se[c("(Intercept)", "x", "y", "z")],
  Loglinear = se_ll[c("v", "x:v", "y:v", "z:v")],
  Difference = c(
    se_ll["v"]-se["(Intercept)"],
    se_ll["x:v"]-se["x"]
  )
)
print(comparison_se)
```
The biggest difference is smaller than \(10^{-6}\) which is negligible.
This confirms that the loglinear model matches the logistic model we chose.

Let \(V=1\) mean child survival, \(V=2\) mean child death.

The logistic model in task 4 has the form:
\[
\operatorname{logit} P(V=1|X=i,Y=j,Z=k) 
= \alpha + \beta_i^X + \beta_j^Y + \beta_k^Z
\]

Consider the loglinear model \((XY, XZ, YZ, V)\). Its formula is:
\[
\ln\mu_{ijkl} =
\lambda
+\lambda_i^X+\lambda_j^Y+\lambda_k^Z+\lambda_l^V
+\lambda_{ij}^{XY}+\lambda_{ik}^{XZ}+\lambda_{il}^{XV}
+\lambda_{jk}^{YZ}+\lambda_{jl}^{YV}+\lambda_{kl}^{ZV}
\]
The logit of \(V\) is:
\[
\begin{align*}
& \operatorname{logit} P(V=1|X=i,Y=j,Z=k) \\
=& \ln \frac{P(V=1|X=i,Y=j,Z=k)}{P(V=0|X=i,Y=j,Z=k)} \\
=& \ln \frac{\mu_{ijk1}}{\mu_{ijk0}} \\
=& \ln \mu_{ijk1} - \ln\mu_{ijk0} \\
=& (
\lambda
+\lambda_i^X+\lambda_j^Y+\lambda_k^Z+\lambda_1^V
+\lambda_{ij}^{XY}+\lambda_{ik}^{XZ}+\lambda_{i1}^{XV}
+\lambda_{jk}^{YZ}+\lambda_{j1}^{YV}+\lambda_{k1}^{ZV}
) \\
- &
(
\lambda
+\lambda_i^X+\lambda_j^Y+\lambda_k^Z+\lambda_2^V
+\lambda_{ij}^{XY}+\lambda_{ik}^{XZ}+\lambda_{i0}^{XV}
+\lambda_{jk}^{YZ}+\lambda_{j0}^{YV}+\lambda_{k0}^{ZV}
) \\
=& (\lambda_1^V - \lambda_2^V)
+ (\lambda_{i1}^{XV} - \lambda_{i0}^{XV})
+ (\lambda_{j1}^{YV} - \lambda_{j0}^{YV})
+ (\lambda_{k1}^{ZV} - \lambda_{k0}^{ZV}) \\
:=& \alpha + \beta_i^X + \beta_j^Y + \beta_k^Z
\end{align*}
\]

Loglinear model:
\[\mu_{ijkl} = E[N_{ijkl}] = \exp(\lambda+\lambda_i^X+\lambda_j^Y+\lambda_k^Z+\lambda_l^V)\]
Loglinear model estimates:
\[\hat\mu_{ijkl} = \exp(\hat\lambda+\hat\lambda_i^X+\hat\lambda_j^Y+\hat\lambda_k^Z+\hat\lambda_l^V)\]

\[
\begin{align*}
&\ln \mu_{ijkl} \\
=& \ln E[N_{ijkl}] \\
=& \lambda \\
& +\lambda_i^X + \lambda_j^Y + \lambda_k^Z + \lambda_l^V \\
& +\lambda_{ij}^{XY} + \lambda_{ik}^{XZ} + \lambda_{il}^{XV} + \lambda_{jk}^{YZ} + \lambda_{jl}^{YV} + \lambda_{kl}^{ZV} \\
& +\lambda_{ijk}^{XYZ} + \lambda_{ijl}^{XYV} + \lambda_{ikl}^{XZV} + \lambda_{jkl}^{YZV} \\
& +\lambda_{ijkl}^{XYZV} \\
\end{align*}
\]

```{r}
data <- data.frame(
  x = c(0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1),
  y = c(1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1),
  z = c(0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1),
  v = c(1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1),
  n = c(40, 315, 9, 50, 6, 24, 459, 4012, 11, 4, 41, 147, 1594, 1, 14, 124)
)
#model4 <- glm(v/n ~ x+y+z, weights=n, family = binomial, data = data)
model4 <- glm(v ~ x + y + z, weights=n, family = binomial(link=logit), data = data)
coefficients <- summary(model4)$coefficients
beta <- coefficients[, "Estimate"]
print(beta)
print("---")

model4_loglinear <- glm(n ~ v + x + y + z + x:z + x:y + y:z + x:y:z + v:z + v:x + v:y + v:x:y, family = poisson(link=log), data = data)
coefficients <- summary(model4_loglinear)$coefficients
beta <- coefficients[, "Estimate"]
print(beta)
print("---")
```

```{r}
formulas <- c(
"n ~ x * y * v + x * z * v + y * z * v + x * y * z + x * y * z * v",
"n ~ x * y * v + x * z * v + y * z * v + x * y * z",
"n ~ x * z * v + y * z * v + x * y * z",
"n ~ y * z * v + x * y * z + x * v",
"n ~ x * y * z + x * v + y * v + z * v",
"n ~ x * v + y * v + z * v + x * z + x * y + y * z",
"n ~ x * v + z * v + x * y",
"n ~ y * v + z * v + x * z + x * y + y * z",
"n ~ z * v + x * z + x * y + y * z",
"n ~ x * z + x * y + y * z + v",
"n ~ x * y + y * z + v",
"n ~ y * z + v + x",
"n ~ x + y + z + v"
  )
for(f in formulas) {
#model4_loglinear <- glm(n ~ x+y+z+v + v*x + v*y + v*z, family = poisson(link=log), data = data)
model4_loglinear <- glm(f, family = poisson(link=log), data = data)
coefficients <- summary(model4_loglinear)$coefficients
beta <- coefficients[, "Estimate"]
se <- coefficients[, "Std. Error"]
#print(beta)
#print(coef(model4_loglinear)[grep("v", names(coef(model4_loglinear)))])
print(sum(coef(model4_loglinear)[grep("z", names(coef(model4_loglinear)))]))
}
```


```{r}
coefficients <- summary(model4_loglinear)$coefficients
beta <- coefficients[, "Estimate"]
se <- coefficients[, "Std. Error"]
print(beta)
#print(se)
```



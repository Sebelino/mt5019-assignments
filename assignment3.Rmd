---
title: "Computer Assignment 3"
author:
  - "Ville Sebastian Olsson (19911231-1999)"
  - "Arvind Guruprasad (19990212-6938)"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: true
  pdf_document:
    fig_caption: true
---

```{r, echo=FALSE, results='hide', fig.show='hide'}
source("assignment3.R")
list2env(ass, envir = .GlobalEnv)
knitr::opts_chunk$set(echo = FALSE, comment = "")
```

# Exercise 3:1

## 1. Model finding

The data comes from a study reported by Wermuth (1976), which was collected in a birth clinic. The dataset includes the following variables:

- **Mother's age (\(X\)):** `<30` and `30+`.
- **Smoking habits (\(Y\)):** `<5 cigarettes/day` and `5+ cigarettes/day`.
- **Gestational age (\(Z\)):** `<260 days` and `â‰¥260 days`.
- **Child survival (\(V\)):** Binary outcome indicating survival (`Yes`/`No`).

Our objective is to find a suitable loglinear model
which strikes the best balance between goodness-of-fit
and model complexity.

```{r}
#trace <- capture.output(step(mstep3_AIC, direction="backward", trace=TRUE, scope = list(upper = mstep3_AIC, lower = m1)))
# trace_table <- data.frame(
#    Formula = grep("n ~ ", trace, value = TRUE)
# )
```

To find a suitable model, we will generate a set of model by following this process:

1. Read the CSV data.
2. Use the data to fit the saturated model, \((XYZV)\).
3. Remove interaction terms one by one, starting with the highest-order interactions.
   For each set of interaction terms, use it to fit a model.
   Each combination must include the main effects, i.e. \(X\), \(Y\), \(Z\) and \(V\).
4. For each sub-model, calculate deviance, Akaike's information criterion (AIC) and p-value with respect to the saturated model.

The deviance is used to calculate the p-value as follows:
\[\text{p-value} = P(\chi^2_{\text{df}} \geq T_{\chi^2})\]
where \(T_{\chi^2} = 2(\ell(XYZ) - \ell(M))\) denotes the deviance and \(\text{df}\)
is the degrees of freedom. \(\ell(M)\) denotes the log-likelihood under model \(M\).

The degrees of freedom is calculated as:
\[\text{df} = IJKL - \text{number of parameters of }M\]
where \(IJKL = 2\cdot2\cdot2\cdot2 = 16\) is the number of cells in the contingency table.

The AIC of a model \(M\) is a measure of the model's balance between goodness-of-fit
and model complexity, where a lower AIC value implies a better model. The AIC is given by:
\[\text{AIC}(M) = -2\ell(M) + 2p\]
where \(p\) is the number of parameters in \(M\),
including intercept terms.

We can use the function `step` to find the model with the lowest AIC.
Since we are supposed to find this model by
starting with the saturated model (`x*y*z`) and then
removing (but not adding)
interaction terms, we must run `step` with `direction="backward"`.

Result:
```{r}
data <- read.csv("data_ca3.csv")
msat <- glm(n ~ x * y * z * v, family = poisson, data = data)
m3 <- step(msat, direction="backward", trace=TRUE, scope = list(upper = n~(x+y+z+v)^4, lower = n~(x+y+z+v)^3))
m2 <- step(m3, direction="backward", trace=TRUE, scope = list(upper = n~(x+y+z+v)^3, lower = n~(x+y+z+v)^2))
m1 <- step(m2, direction="backward", trace=TRUE, scope = list(upper = n~(x+y+z+v)^2, lower = n~x+y+z+v))
```

TODO: but higher interaction terms should be
removed before lower-interaction terms, and that's
not happening here.
The 2-way `y:v` term is removed before any of the remaining 3-way terms:
<pre><code>
n ~ x + y + z + v + x:y + x:z + y:z + x:v + z:v + y:v + x:y:v + x:z:v + y:z:v + x:y:z + x:y:z:v
n ~ x + y + z + v + x:y + x:z + y:z + x:v + z:v + y:v + x:y:v + x:z:v + y:z:v + x:y:z
n ~ x + y + z + v + x:y + x:z + y:z + x:v + z:v + y:v + x:y:v + x:z:v + y:z:v
n ~ x + y + z + v + x:y + x:z + y:z + x:v + z:v + <span style="background-color: yellow">y:v</span> + x:y:v + x:z:v
n ~ x + y + z + v + x:y + x:z + x:v + y:v + z:v + x:y:v + x:z:v
n ~ x + y + z + v + x:y + x:z + x:v + y:v + z:v + x:y:v
n ~ x + y + z + v + x:y + x:z + x:v + y:v + z:v
</code></pre>


Result:
```{r}
model_table
```
TODO: Ask about whether to list all combinations of interaction terms vs. total ordering of models, where set n is a subset of set n-1

## 2. Model selection

The table contains the following information which helps us
select the best model:

**1. Deviance**

- Measures the difference in goodness-of-fit between the submodel and the saturated model.
- A low deviance indicates a good fit.

**2. Degrees of Freedom**

- Reflects the number of parameters removed from the saturated model.
- Higher df means a simpler model.

**3. p-value**

- Tests whether the submodel is significantly worse than the saturated model.
- If \( p > 0.05 \), the simpler model is adequate.

**4. AIC**

- Evaluates the trade-off between model complexity and fit.
- Select the model with the lowest AIC.

```{r}
model_table[order(model_table$AIC),]
```

The table above has been sorted by AIC in ascending order,
to gauge which of the simpler model would be effective with
respect to the saturated model.

We should select the model that satisfies two criteria:

* Minimal AIC value
* The p-value is greater than 0.05

Based on the table above, we want to select model 6,
which includes the interaction terms `XV, YV, ZV, XZ, XY, YZ`.

# 3. Model interpretation

The model we selected looks like this:

```{r}
best_model
```

Given the lack of three-way interactions, this model
is much simpler relative to the relative model and the
other three-way interaction model while retaining
its ability to fit the data reasonably well.

```{r}
associations
```

\[OR_Y = \frac{odds(Y=1)}{odds(Y=0)}\]

Critical Factors:

- **The main effects of \( Y \) (smoking) and \( Z \) (gestational age)** are significant predictors of child survival.
- **The interaction \( V : Z \) (child survival and gestational age)** has the strongest effect, highlighting the importance of adequate gestational periods for child survival.

Non-significant Effects:

- The interactions \( V : Y \), \( X : Z \), and \( Y : Z \) are not statistically significant, suggesting these effects are not strong enough to influence survival outcomes in this dataset.

Implications:

- Reducing smoking among mothers and improving gestational age outcomes are likely to have the most significant positive impact on child survival rates.


# 4. Logistic model

We now want to fit a logistic model
where \(V\) (child survival) is a response variable.
This model would then help us determine
how the mother's age, smoking habits and gestational
age affect the chances of the child surviving.

As before, we start by considering the saturated model.
\(V\) is now a response variable, so the saturated model has the formula:
\[V \sim X+Y+Z+X:Y+X:Z+Y:Z+X:Y:Z\]
We then use the `step` function with
`direction="backward"` to explore
the model space by removing one interaction
term at a time.
This may continue until the main effects model,
\(V\sim X+Y+Z\), is found.

**TODO: Are we finding the best model incorrectly here as well?**

Output:

```{r}
data <- read.csv("data_ca3.csv")
msat <- glm(v ~ x * y * z, weights=n, family = binomial, data = data)
mstep_AIC<-step(msat, direction="backward", trace=TRUE, scope = list(upper = v~x*y*z, lower = v~x+y+z))
```
We see that the model with the lowest AIC (`1092.76`)
is the main effects model, \(V \sim X+Y+Z\).
We can compare it to the model
with the second-lowest AIC value (`1094.22`), i.e. \(V \sim X+Y+Z+X:Z\):

```{r}
second_simplest_model <- glm(v ~ x + y + z + x:z, weights=n, family = binomial, data = data)
anova(mstep_AIC, second_simplest_model, test = "LRT")
```

The more complex model has a slightly lower residual deviance
(`1084.2` vs. `1084.8`), indicating a marginally better fit
at the cost of higher model complexity.

\[\text{p-value} = P(T_{\chi^2,1} \geq 0.53951) = 0.4626\]
where \(T_{\chi^2,1}\) denotes the chi-squared likelihood ratio test statistic
with 1 degree of freedom.
The p-value is greater than 0.05, so
there is no evidence to suggest that the added \(X:Z\) term
improves the model's fit significantly.
Hence, we choose the simpler main effects model.

The main effects model has the following coefficients:
```{r}
summary(mstep_AIC)
```

Based on these, we calculate the odds ratio and associated confidence intervals:

```{r}
# Extract coefficients
coefficients4 <- summary(mstep_AIC)$coefficients
beta4 <- coefficients4[, "Estimate"]
se4 <- coefficients4[, "Std. Error"]

# Compute odds ratios and confidence intervals
odds_ratios4 <- exp(beta4)
lower_ci4 <- exp(beta4 - 1.96 * se4)
upper_ci4 <- exp(beta4 + 1.96 * se4)

# Combine results into a data frame
results4 <- data.frame(
  Odds_Ratio = odds_ratios4,
  Lower_CI = lower_ci4,
  Upper_CI = upper_ci4,
  P_Value = coefficients4[, "Pr(>|z|)"]
)
print(results4)
```

## Mother's age

The estimated odds ratio of \(0.627\) for \(X\) is less than 1,
suggesting that older mothers have lower odds of child survival.

The p-value is \(0.00954\) which is less than \(0.05\).
Hence, we reject the null hypothesis that the odds ratio is 1.
We are 95 \% confident that the true odds ratio lies in the
interval \([0.440, 0.892]\).

This result may make sense intuitively, as
older mothers may have more health issues that could
affect the child's survival chances.
However, 30 is arguably not a high enough threshold
for classifying mothers based on age-related risks.
If there were more age categories, or if \(X\)
were an interval variable, we might have been able to
make a more nuanced conclusion.

## Gestational age

The estimated odds ratio of \(27.4\) for \(Z\) is greater than 1,
suggesting that children born after 260 days of gestation have higher odds of survival compared to those born before 260 days.
The odds are \(27.4\) times higher for children born after 260 days compared to those born before 260 days.

The p-value is much less than 0.05, meaning that we
reject the null hypothesis that the odds is 1 with strong
statistical significance.
We are 95 \% confident that the true odds ratio lies
in the interval \([19.1,39.3]\).

This result is hardly surprising, as children born prematurely
may be miscarriages or premature births
where the vital organs of the infant may be underdeveloped.

## Smoking habits

The estimated odds ratio for the smoking risk factor (\(Y\)) indicates that mothers
who smoke 5+ cigarettes per day
have 65.5% of the odds of child survival compared to mothers who smoke less
than 5 cigarettes per day, holding all other variables constant.
Since the odds ratio is less than 1, it suggests a negative association between smoking heavily and child survival.

However, the p-value exceeds 0.05, so we cannot conclude that the effect of smoking is statistically significant.
We are 95 \% confident that the true odds ratio lies in the interval \([0.391, 1.10]\). This interval includes 1, which is consistent with our conclusion regarding statistical significance.

Intuitively, it makes sense that smoking would negatively affect child survival, as smoking is commonly regarded to be associated with
negative health outcomes in general.
Hence, the fact that our estimate for the odds ratio is lower
than 1 makes sense.
The fact that our result is not statistically significant
could be due to the small sample size
or the fact that the effect of smoking on survival
is mediated or overshadowed by other factors, such as gestational age (\(Z\)) which has a very strong effect.

## Intercept

The estimated odds ratio of \(6.13\) for the intercept means
that the odds of child survival are \(6.13\) times higher
than the odds of the child not surviving
when all explanatory variables are at their baseline levels,
i.e. when \(X=0\), \(Y=0\), and \(Z=0\).
This means that a mother who is below 30 years old, smokes less than 5 cigarettes per day, and has a gestational age of less than 260 days has
\(6.13\) better odds of her child surviving compared to it not surviving.

The associated p-value is much less than 0.05, meaning that we
reject the null hypothesis that the odds is 1 with strong
statistical significance.
We are 95 \% confident that the true odds ratio lies
in the interval \([4.71, 7.99]\).

It may be challenging to tell whether this
estimated odds ratio makes sense intuitively.
While a shorter gestational age (\(Z=0\))
may be a risk factor, the lack of smoking (\(X=0\))
and being below 30 years old (\(Y=0\)) might
be protective factors that offset this risk.
Most children in this study survived
(`r sum(data[data$v==1,])` vs. `r sum(data[data$v==0,])`).
Hence, it does not seem unreasonable
that one child of this kind dies for each six children
of the same kind who survive.

**TODO we can prolly remove the below**

```{r}
model_table4
```

Based on the table, AIC is minimized for the
main effects model \((X, Y, Z)\)
(with AIC: `1092.757`),
so we consider it the model that strikes the best balance
between goodness-of-fit and model complexity.

```{r}
results4
```

# 5. Logistic-loglinear model relationship

In problem 4, we selected the logistic regression model with the lowest AIC
value, namely:
\[V \sim X + Y + Z\]

There exists a corresponding loglinear model that
yields identical estimates and standard errors for the corresponding parameters.
The model must include a specific set of interaction terms.

There are \(m=4\) binary variables involved, namely \(X\), \(Y\), \(Z\), and \(V\). 
The loglinear model must contain the terms of the following types:

* A: The same terms as the logistic model, in this case \(X\), \(Y\), and \(Z\)
* B: All possible interactions of order \(m-1=3\) and lower between the three explanatory variables, in this case \(X:Y\), \(X:Z\), \(Y:Z\), and \(X:Y:Z\)
* C: The response variable \(V\)
* D: Interactions between the response variable \(V\) and each term in the logistic model,
i.e. \(X:V\), \(Y:V\), and \(Z:V\)

Hence, we end up with the following formula:
\[
N \sim
\underbrace{X + Y + Z}_{A} +
\underbrace{X:Y + X:Z + Y:Z + X:Y:Z}_{B} +
\underbrace{V}_{C} +
\underbrace{X:V + Y:V + Z:V}_{D}
\]

Upon fitting both models,
we should expect the coefficients listed in Table 1 to have equal values.

| Logistic model | Loglinear model |
|:--------------:|:---------------:|
| Intercept      | \(V\)           |
| \(X\)          | \(X:V\)         |
| \(Y\)          | \(Y:V\)         |
| \(Z\)          | \(Z:V\)         |
<div style="text-align: center;">
*Table 1: Each term in the logistic model and its matching term in the loglinear model.*
</div>

## Estimates

Fitting the logistic model yields the following estimates:

```{r}
data <- data.frame(
  x = c(0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1),
  y = c(1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1),
  z = c(0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1),
  v = c(1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1),
  n = c(40, 315, 9, 50, 6, 24, 459, 4012, 11, 4, 41, 147, 1594, 1, 14, 124)
)
model4 <- glm(v ~ x + y + z, weights=n, family = binomial(link=logit), data = data)
coefficients <- summary(model4)$coefficients
beta <- coefficients[, "Estimate"]
print(beta)
```
Fitting the equivalent loglinear model yields the following estimates:
```{r}
# https://stats.stackexchange.com/questions/476742/cant-find-loglinear-models-corresponding-logistic-regression-model
# https://teaching.sociology.ul.ie/SSS/lugano/node58.html
model4_loglinear <- glm(n ~ x + y + z + v
                        + x:y
                        + x:z
                        + x:v
                        + y:z
                        + y:v
                        + z:v
                        + x:y:z
                        ,
                        family = poisson(link=log), data = data)
coefficients_ll <- summary(model4_loglinear)$coefficients
beta_ll <- coefficients_ll[, "Estimate"]
print(beta_ll)
```
When comparing each pair of terms according to Table 1,
we see that each pair of coefficient values are virtually identical:
```{r}
comparison <- data.frame(
  LogisticParam = c("(Intercept)","x","y","z"),
  LoglinearParam=c("v","x:v","y:v","z:v"),
  Logistic = beta[c("(Intercept)", "x", "y", "z")],
  Loglinear = beta_ll[c("v", "x:v", "y:v", "z:v")],
  Difference = c(
    beta_ll["v"]-beta["(Intercept)"],
    beta_ll["x:v"]-beta["x"],
    beta_ll["y:v"]-beta["y"],
    beta_ll["z:v"]-beta["z"]
  ),
  row.names=NULL
)
print(comparison)
```
The biggest difference is smaller than \(10^{-10}\).
This difference is negligible and likely due to loss of numerical precision
when the values were computed.

TODO: check significance p-value of models selected by step

## Standard errors
Calculating the standard errors yields similar results.

Standard errors in the logistic model:
```{r}
se <- coefficients[, "Std. Error"]
print(se)
```
Standard errors in the loglinear model:
```{r}
se_ll <- coefficients_ll[, "Std. Error"]
print(se_ll)
```
Again, we see that the standard errors are virtually identical:
```{r}
comparison_se <- data.frame(
  LogisticParam = c("(Intercept)","x","y","z"),
  LoglinearParam=c("v","x:v","y:v","z:v"),
  Logistic = se[c("(Intercept)", "x", "y", "z")],
  Loglinear = se_ll[c("v", "x:v", "y:v", "z:v")],
  Difference = c(
    se_ll["v"]-se["(Intercept)"],
    se_ll["x:v"]-se["x"]
  )
)
print(comparison_se)
```
The biggest difference is smaller than \(10^{-6}\) which is negligible.
This confirms that the loglinear model matches the logistic model we chose.

Let \(V=1\) mean child survival, \(V=2\) mean child death.

The logistic model in task 4 has the form:
\[
\operatorname{logit} P(V=1|X=i,Y=j,Z=k) 
= \alpha + \beta_i^X + \beta_j^Y + \beta_k^Z
\]

Consider the loglinear model \((XY, XZ, YZ, V)\). Its formula is:
\[
\ln\mu_{ijkl} =
\lambda
+\lambda_i^X+\lambda_j^Y+\lambda_k^Z+\lambda_l^V
+\lambda_{ij}^{XY}+\lambda_{ik}^{XZ}+\lambda_{il}^{XV}
+\lambda_{jk}^{YZ}+\lambda_{jl}^{YV}+\lambda_{kl}^{ZV}
\]
The logit of \(V\) is:
\[
\begin{align*}
& \operatorname{logit} P(V=1|X=i,Y=j,Z=k) \\
=& \ln \frac{P(V=1|X=i,Y=j,Z=k)}{P(V=0|X=i,Y=j,Z=k)} \\
=& \ln \frac{\mu_{ijk1}}{\mu_{ijk0}} \\
=& \ln \mu_{ijk1} - \ln\mu_{ijk0} \\
=& (
\lambda
+\lambda_i^X+\lambda_j^Y+\lambda_k^Z+\lambda_1^V
+\lambda_{ij}^{XY}+\lambda_{ik}^{XZ}+\lambda_{i1}^{XV}
+\lambda_{jk}^{YZ}+\lambda_{j1}^{YV}+\lambda_{k1}^{ZV}
) \\
- &
(
\lambda
+\lambda_i^X+\lambda_j^Y+\lambda_k^Z+\lambda_2^V
+\lambda_{ij}^{XY}+\lambda_{ik}^{XZ}+\lambda_{i0}^{XV}
+\lambda_{jk}^{YZ}+\lambda_{j0}^{YV}+\lambda_{k0}^{ZV}
) \\
=& (\lambda_1^V - \lambda_2^V)
+ (\lambda_{i1}^{XV} - \lambda_{i0}^{XV})
+ (\lambda_{j1}^{YV} - \lambda_{j0}^{YV})
+ (\lambda_{k1}^{ZV} - \lambda_{k0}^{ZV}) \\
:=& \alpha + \beta_i^X + \beta_j^Y + \beta_k^Z
\end{align*}
\]

Loglinear model:
\[\mu_{ijkl} = E[N_{ijkl}] = \exp(\lambda+\lambda_i^X+\lambda_j^Y+\lambda_k^Z+\lambda_l^V)\]
Loglinear model estimates:
\[\hat\mu_{ijkl} = \exp(\hat\lambda+\hat\lambda_i^X+\hat\lambda_j^Y+\hat\lambda_k^Z+\hat\lambda_l^V)\]

\[
\begin{align*}
&\ln \mu_{ijkl} \\
=& \ln E[N_{ijkl}] \\
=& \lambda \\
& +\lambda_i^X + \lambda_j^Y + \lambda_k^Z + \lambda_l^V \\
& +\lambda_{ij}^{XY} + \lambda_{ik}^{XZ} + \lambda_{il}^{XV} + \lambda_{jk}^{YZ} + \lambda_{jl}^{YV} + \lambda_{kl}^{ZV} \\
& +\lambda_{ijk}^{XYZ} + \lambda_{ijl}^{XYV} + \lambda_{ikl}^{XZV} + \lambda_{jkl}^{YZV} \\
& +\lambda_{ijkl}^{XYZV} \\
\end{align*}
\]

```{r}
data <- data.frame(
  x = c(0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1),
  y = c(1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1),
  z = c(0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1),
  v = c(1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1),
  n = c(40, 315, 9, 50, 6, 24, 459, 4012, 11, 4, 41, 147, 1594, 1, 14, 124)
)
#model4 <- glm(v/n ~ x+y+z, weights=n, family = binomial, data = data)
model4 <- glm(v ~ x + y + z, weights=n, family = binomial(link=logit), data = data)
coefficients <- summary(model4)$coefficients
beta <- coefficients[, "Estimate"]
print(beta)
print("---")

model4_loglinear <- glm(n ~ v + x + y + z + x:z + x:y + y:z + x:y:z + v:z + v:x + v:y + v:x:y, family = poisson(link=log), data = data)
coefficients <- summary(model4_loglinear)$coefficients
beta <- coefficients[, "Estimate"]
print(beta)
print("---")
```

```{r}
formulas <- c(
"n ~ x * y * v + x * z * v + y * z * v + x * y * z + x * y * z * v",
"n ~ x * y * v + x * z * v + y * z * v + x * y * z",
"n ~ x * z * v + y * z * v + x * y * z",
"n ~ y * z * v + x * y * z + x * v",
"n ~ x * y * z + x * v + y * v + z * v",
"n ~ x * v + y * v + z * v + x * z + x * y + y * z",
"n ~ x * v + z * v + x * y",
"n ~ y * v + z * v + x * z + x * y + y * z",
"n ~ z * v + x * z + x * y + y * z",
"n ~ x * z + x * y + y * z + v",
"n ~ x * y + y * z + v",
"n ~ y * z + v + x",
"n ~ x + y + z + v"
  )
for(f in formulas) {
#model4_loglinear <- glm(n ~ x+y+z+v + v*x + v*y + v*z, family = poisson(link=log), data = data)
model4_loglinear <- glm(f, family = poisson(link=log), data = data)
coefficients <- summary(model4_loglinear)$coefficients
beta <- coefficients[, "Estimate"]
se <- coefficients[, "Std. Error"]
#print(beta)
#print(coef(model4_loglinear)[grep("v", names(coef(model4_loglinear)))])
print(sum(coef(model4_loglinear)[grep("z", names(coef(model4_loglinear)))]))
}
```


```{r}
coefficients <- summary(model4_loglinear)$coefficients
beta <- coefficients[, "Estimate"]
se <- coefficients[, "Std. Error"]
print(beta)
#print(se)
```



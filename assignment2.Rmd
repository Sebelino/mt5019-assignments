---
title: "Computer Assignment 2"
author:
  - "Ville Sebastian Olsson (19911231-1999)"
  - "Arvind Guruprasad (19990212-6938)"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: true
  pdf_document:
    fig_caption: true
---

```{r, echo=FALSE, results='hide', fig.show='hide'}
source("assignment2.R")
list2env(ass, envir = .GlobalEnv)
knitr::opts_chunk$set(echo = FALSE, comment = "")
```

# Exercise 2:1

## 1. \(\beta\) model

The following parameters in the model can be estimated:

* The intercept \(\beta_0\), which represents the log-odds of having periodontitis when
\(x=0\) (i.e. no floss use).
This parameter can be directly estimated from the data because the proportions of periodontitis cases for \(x=0\) are provided by the table.

* The effect parameter \(\beta_1\), which represents the change in log-odds
of having periodontitis between those who use floss (\(x=1\)) and those who
do not (\(x=0\)). This parameter can also be directly estimated because we have the counts for both groups.

Estimates \(\hat\beta_0\) and \(\hat\beta_1\) for these are given by fitting
a logistic regression model and fetching the coefficients of the model.

Coefficients of the model:
```{r}
coef(model21)
```
We will discuss these estimates below.

### Intercept parameter
The estimate for \(\beta_0\) is \(\hat\beta_0\approx -0.58\).
The negative value suggests that
it is more likely for a non-floss user to not have periodontitis than
to have it.
To interpret \(\hat\beta_0\) further,
we can first exponentiate the log-odds to obtain the odds:

\[\text{odds} = e^{\hat\beta_0}\]
which yields the following value:
```{r}
exp(coef(model21)["(Intercept)"])
```

Meaning, for non-floss users, the odds of having periodontitis are about 0.558.
In other words, for every 100 non-floss users who do not have periodontitis, around 56 are expected to have it.

### Effect parameter
The estimate for \(\beta_1\) is \(\hat\beta_1\approx -0.64\).
The negative value suggests that floss users are less likely to have
have periodontitis.

Exponentiating this coefficient gives the odds ratio for floss users relative to non-users:

\[\theta = e^{\hat\beta_1}\]
which yields:
```{r}
exp(coef(model21)["useYes"])
```

This means that floss users have approximately 52.5% of the odds of having periodontitis compared to non-users, suggesting a protective effect of flossing against periodontitis.

## 2. \(\gamma\) model

The following parameters in the model can be estimated:

* The intercept \(\gamma_0\), which represents the log-odds of using dental floss when
\(y=0\) (i.e. no periodontitis).
This parameter can be directly estimated from the data because the proportions of floss uses for \(y=0\) are provided by the table.

* The effect parameter \(\gamma_1\), which represents the change in log-odds
of being a floss user between those with periodontitis (\(y=1\)) and those without it (\(y=0\)). This parameter can also be directly estimated because we have the counts for both groups.

Estimates \(\hat\gamma_0\) and \(\hat\gamma_1\) for these are given by fitting
a logistic regression model and fetching the coefficients of the model.

Coefficients of the model:
```{r}
coef(model21b)
```

### Intercept parameter

The estimate for \(\gamma_0\) is \(\hat\gamma_0\approx -1.3\).
The negative value suggests that
it is more likely for a healthy individual to not use dental floss than
to do it.
To interpret \(\hat\gamma_0\) further,
we can first exponentiate the log-odds to obtain the odds:

\[\text{odds} = e^{\hat\gamma_0}\]
which yields the following value:
```{r}
exp(coef(model21b)["(Intercept)"])
```

Meaning, for healthy individuals, the odds of using floss are about 0.28.
In other words, for every 100 healthy individuals who do not floss, around 28 are expected to floss.

### Effect parameter

The estimate for \(\gamma_1\) is \(\hat\gamma_1\approx -0.64\).
The negative value indicates that individuals with periodontitis (\(y=1\)) are less likely to use dental floss compared to healthy individuals (\(y=0\)).

Exponentiating this coefficient gives the odds ratio for flossing behavior among individuals with periodontitis compared to healthy individuals.

\[\theta = e^{\hat\beta_1}\]
which yields:
```{r}
exp(coef(model21)["useYes"])
```

This means that the odds of using dental floss for individuals with periodontitis are approximately 53 % of the odds for healthy individuals.
This result suggests a behavioral pattern where individuals with periodontitis are less likely to have used dental floss, possibly because abstaining from flossing increases the risk of periodontitis.

## 3. Model comparison

The estimate for \(\beta_0\) is the log-odds of periodontitis for non-floss users, calculated as follows:

\[
\beta_0 = \operatorname{logit}(P(\text{periodontitis} | x=0))
= \ln \left( \frac{P(\text{periodontitis} | x=0)}{1-P(\text{periodontitis} | x=0)}\right)
= \ln \left(\frac{\frac{148}{413}}{1-\frac{148}{413}}\right)
\approx -0.58
\]

The estimate for \( \beta_1 \) is the difference in log-odds of periodontitis between floss users and non-floss users, calculated as follows:

\[
\beta_1 = \ln \left( \frac{\text{odds}_{x=1}(\text{periodontitis})}{\text{odds}_{x=0}(\text{periodontitis})} \right)
= \ln \left( \frac{22/75}{148/265} \right)
\approx -0.64
\]

Numerically:
```{r,echo=TRUE,results='show'}
odds_floss <- 22 / 75
odds_no_floss <- 148 / 265
log_odds_ratio <- log(odds_floss / odds_no_floss)
log_odds_ratio
```

The estimate for \(\gamma_0\) is the log-odds of flossing for healthy individuals,
calculated as follows:

\[
\gamma_0 = \operatorname{logit}(P(\text{floss} | y=0))
= \ln \left( \frac{P(\text{floss} | y=0)}{1-P(\text{floss} | y=0)}\right)
= \ln \left(\frac{\frac{75}{340}}{1-\frac{75}{340}}\right)
\approx -1.3
\]

The estimate for \( \gamma_1 \) is the difference in log-odds of flossing between periodontitis cases and healthy individuals, calculated as follows:

\[
\gamma_1 = \ln \left( \frac{\text{odds}_{y=1}(\text{floss})}{\text{odds}_{y=0}(\text{floss})} \right)
= \ln \left( \frac{22/148}{75/265} \right)
\approx -0.64
\]

Numerical result:
```{r,echo=TRUE,results='show'}
odds_periodontitis <- 22 / 148
odds_no_periodontitis <- 75 / 265
log_odds_ratio <- log(odds_periodontitis / odds_no_periodontitis)
log_odds_ratio
```

We observe that \(\gamma_1=\beta_1\approx -0.64\).
This equality holds because both parameters represent
the same log-odds-ratio, but computed differently:
\[e^{\beta_1} = \frac{\text{odds}_{x=1}(\text{periodontitis})}{\text{odds}_{x=0}(\text{periodontitis})} = \frac{22/75}{148/265} = \frac{22/148}{75/265} = \frac{\text{odds}_{y=1}(\text{floss})}{\text{odds}_{y=0}(\text{floss})} = e^{\gamma_1}\]
This simplification relies on the inherent symmetry of odds ratios in a 2Ã—2 table, where the same odds ratio is obtained regardless of whether the roles of the predictor and outcome are reversed.

# Exercise 2:2

We are given data of 165 mice where various doses of a particular drug (bensopyren) is administered over a 10 month period. The objective is to examine the count of mice that has developed lung tumor during this timeframe.

```{r, results='asis'}
cat("
\\[
\\begin{array}{|c|c|c|c|c|c|c|c|c|}
\\hline
\\text{log(dose)} & -7.60 & -6.22 & -4.60 & -3.00 & -1.39 & 0.92 & \\Sigma \\\\
\\hline
\\text{Tumor} & 1 & 2 & 4 & 9 & 12 & 32 & 60 \\\\
\\hline
\\text{No tumor} & 17 & 17 & 24 & 23 & 16 & 8 & 105 \\\\
\\hline
\\Sigma & 18 & 19 & 28 & 32 & 28 & 40 & 165 \\\\
\\hline
\\end{array}
\\]
")
```

## 1. Estimate risk and log-odds

Let \(n_{1d}\) denote the observed number of mice with tumors at log-dose \(d\).
Let \(n_{2d}\) denote the observed number of mice with no tumor at log-dose \(d\).

For each log-dose \(d\), the estimated risk of developing a tumor is given by the proportion of mice with tumors at that log-dose level, i.e.:

\[
\hat\pi_d = \frac{n_{1d}}{n_{+d}}
\]
where \(n_{+d} = n_{1d}+n_{2d}\).

The estimated log-odds for each log-dose level can be calculated as:
\[
\text{Log-Odds} = \ln \left( \frac{\pi_{1d}}{1 - \pi_{1d}} \right)
\]

We exponentiate the log-dose to obtain the actual quantity
of the dose.
We end up with the following estimated risks
and log-odds:

```{r}
data22
```

Plot of the risk against the log-dose:

```{r, align='center'}
plot_risk(log_dose, risk)
```

This plot shows the risk of developing a tumor at different log-dose levels.
As the log-dose increases, the risk of developing a tumor also increases. This suggests a positive association between dose and tumor risk: higher doses are associated with a greater likelihood of developing a tumor.
The increase in risk appears to follow a smooth trend, becoming especially steep for higher log-dose values.

Next, we plot the log-odds against the log-dose:

```{r, align='center'}
plot_odds(log_dose, log_odds)
```

This plot shows the relationship between the log-dose and the log-odds of developing a tumor.
The log-odds appear to increase in an approximately linear fashion as log-dose increases. The points follow a roughly straight line, which is a key indicator of a linear relationship.
This linear trend in the log-odds plot suggests that a logistic regression model may be appropriate for modeling the relationship between log-dose and the probability of developing a tumor. In logistic regression, we assume that the log-odds of the outcome is linearly related to the explanatory variable, and this plot supports that assumption.

Since the log-odds plot shows a roughly linear trend with log-dose, a logistic regression model seems appropriate for these data. This model would allow us to quantify the effect of dose on the probability of developing a tumor.

## 2. Fit a Logistic Regression Model

We now fit a logistic regression model to the data
to estimate the relationship between the log-dose (\(X\)) and the log-odds of developing a tumor (\(Y=1\)).
We can represent this relationship as:

\[\text{logit} P(Y = 1 | X=x) = \beta_0+\beta_1 x\]

where

* \(Y\) is a binary response variable where \(Y=0\) means no tumor,
* \(X\) is an explanatory variable (interval variable)
* \(x\) is a realization thereof,
* \(P(Y=1|X=x)\) is the probability of developing a tumor
given a log-dose of \(x\),
* \(\beta_0\) is the intercept parameter,
* \(\beta_1\) is the slope parameter.

Equivalently, we can express the corresponding risk by
transforming the equality into:

\[
P(Y = 1 | X=x) = \frac{1}{1 + e^{-\left(\beta_0 + \beta_1 x\right)}}
\]

\(\hat\beta_0\) denotes our estimate of the intercept parameter. It represents the log-odds of developing a tumor when the log-dose is zero \(X=0\). Although it has a mathematical interpretation, it is possible that it does not have a direct practical meaning in this context, especially if \(\log(\text{dose}) = 0\) (i.e. \(\text{dose}=1\)) is outside the range of observed values.

\(\hat\beta_1\): The slope tells us how the log-odds of developing a tumor change with each unit increase in log(dose). If \(\hat\beta_1\) is positive, this implies that as the log-dose increases, the log-odds of developing a tumor also increase, indicating a higher probability of tumor development at higher doses.

We obtain the estimates of the intercept and slope parameter as follows:

```{r}
coef(model22)
```

By exponentiating the intercept, we obtain its odds:
```{r}
exp(coef(model22)["(Intercept)"])
```
That is, at a log-dose of zero, the odds of developing a tumor are approximately \(1.99\).
Equivalently, at a dose of 1, the odds of developing
a tumor is just below 2:1.

By exponentiating the slope, we obtain its odds:

```{r}
exp(coef(model22)["log_dose"])
```
The positive slope parameter value
of \( `r round(coef(model22)["log_dose"],4) ` \) indicates
a strong association between increasing dose levels and a higher likelihood of tumor development.
The odds value of
\(`r round(exp(coef(model22)["log_dose"]),4)`\)
means that for each one-unit increase in log(dose), the odds of developing a tumor increase by approximately 68.2%.
In other words, higher doses are associated with a significantly higher risk of developing a tumor.

## 3. Covariance Matrix and Confidence Interval

### Covariance matrix

The covariance matrix expresses
the variances of the model coefficients
\(\beta_0\) and \(\beta_1\)
and their covariances.
We can extract these directly from the
model using `vcov`, which yields:

```{r}
cov_matrix
```
The diagonal of the covariance matrix
represents the variance of the intercept and the log dose,
respectively.
The off-diagonal elements represent the covariance.
This value indicates the degree to which the intercept and slope vary together.
The off-diagonal values are not zero, implying that
the parameters are not independent.

We can study this relationship more thoroughly
using the correlation coefficient.
It is given by:
\[
\rho = \frac{\text{Cov}(\beta_0,\beta_1)}{\sqrt{\text{Var}(\beta_0)\cdot \text{Var}(\beta_1) }}
\]

Plugging in the values above, we obtain the
following value for \(\rho\):

```{r}
correlation
```
A correlation of approximately 
\(`r round(correlation,2)`\)
indicates a moderate positive correlation between
the intercept and slope estimates.
This suggests that the intercept and slope estimates are not completely independent; as one estimate increases, the other tends to increase as well.
This is consistent with our conclusion earlier.

### Confidence interval for the intercept
Next, we want to find a likely range for our
estimates of the parameter values.
A 95 \% confidence interval for \(\beta_0\) is given by:
```{r}
conf_intervals["(Intercept)",]
```

The results show that we are 95% confident that the true value of the intercept parameter \(\beta_0\) lies between 
approximately
`r round(conf_intervals["(Intercept)",]["2.5 %"],2)`
and
`r round(conf_intervals["(Intercept)",]["97.5 %"],2)`.
The intercept \(\beta_0\)
represents log-odds, so we can find a
confidence interval for the corresponding odds by
exponentiating both limits of the interval, yielding:
```{r}
exp(conf_intervals["(Intercept)",])
```
This tells us that at a log-dose of zero (i.e. a dose of 1),
the true value of the odds of developing a tumor is
between approximately
`r round(exp(conf_intervals["(Intercept)",]["2.5 %"]),2)`
and
`r round(exp(conf_intervals["(Intercept)",]["97.5 %"]),2)`
with 95 \% confidence.

### Confidence interval for the slope
A 95 \% confidence interval for \(\beta_1\) is given by:
```{r}
conf_intervals["log_dose",]
```

The results show that we are 95% confident that the true value of the slope parameter \(\beta_1\) lies between 0.35 and 0.69.
The slope represents the change in log-odds of developing a tumor for each one-unit increase in log-dose.
We can find
a confidence interval for the corresponding odds ratio by exponentiating
both limits of the interval, yielding:
```{r}
exp(conf_intervals["log_dose",])
```
This tells that for each one-unit increase in log-dose, the odds of developing a tumor are between approximately 1.42 and 1.99 times higher with 95% confidence.
This is consistent with our earlier findings:
an increase in dosage is associated with higher chance of developing a tumor.

### Confidence interval at dose 0.25

We will now use the model to predict the risk of developing a tumor at a log-dose of -1.39.
Using on our formula for the tumor risk earlier,
this probability can be expressed as:

\[
\hat{p} = \widehat{P}(Y = 1 | X=-1.39) = \frac{1}{1 + e^{-\left(\hat\beta_0 + \hat\beta_1 \cdot (-1.39)\right)}}
\]

Since we also know the standard error, we can obtain
the limits of the confidence interval for the tumor risk:

```{r}
probability_ci
```
This interval tells us that we are 95% confident that the true probability of developing a tumor at \(\log(\text{dose}) = -1.39\) lies between approximately 0.39 (39%) and 0.59 (59%).
Since this interval is fairly wide, it reflects some uncertainty in the exact probability estimate but still suggests a moderate to high likelihood of tumor development at this dose level.

## 4. Wald Test

We aim to test the null hypothesis:
$$
H_0: \beta_1 = 0
$$
against the alternative hypothesis:
$$
H_1: \beta_1 \neq 0
$$

Recall that the estimate for \(\beta_1\) we
found earlier was:
```{r}
beta1
```
The associated estimated standard error for \(\beta_1\) is:
```{r}
se_beta1
```

The Wald test statistic (squared variant) is given by:
\[T_{W^2} = \frac{(\hat\beta_1-0)^2}{\operatorname{se}(\hat\beta_1)^2}
=\frac{\hat\beta_1^2}{\operatorname{se}(\hat\beta_1)^2}
\]
with realization:
\[t_{W^2} =\frac{0.5204^2}{0.08502^2}\]

where \( \hat{\beta}_1 \) is the estimated coefficient and \( \operatorname{se}(\hat{\beta}_1) \) is its standard error. The test statistic follows a \(\chi^2(q)\) distribution under the null hypothesis, where \(q=1\) is the degrees of freedom,
which is equal to the number of parameter constraints under \(H_0\).

Numerically computed value for the Wald statistic:
```{r}
wald_statistic
```

We can calculate the p-value from the value of the statistic by the relationship:

\[
p\text{-value} = P(T_{W^2} \geq 37.457)
\]

which yields the following numerically computed p-value:
```{r}
p_value24
```

The p-value is much lower than 0.05,
indicating strong evidence against the null hypothesis.
In other words, the slope \(\beta_1\) is significantly different from zero,
meaning that the dose has significant effect on the odds of developing
a tumor.

## 5. Variance and Sample Size Relationship

We aim to investigate the relationship between the sample size and the variances of parameter estimates in logistic regression. Specifically, we scale the sample size by factors of 10, 100, and 1000,
train a new model on each sample size,
extract the standard errors of both parameters,
and finally
compare the square of the standard errors (i.e. variances).

Numerically computed results:
```{r}
variance_results
```

The intercept variance is approximately:

- Unscaled: \( 6.63 \times 10^{-2} \),
- At a scale factor of 10: \( 6.63 \times 10^{-3} \),
- At a scale factor of 100: \( 6.63 \times 10^{-4} \),
- At a scale factor of 1000: \( 6.63 \times 10^{-5} \).

We see that the variance for the intercept decreases by a factor of 10 for each scale increase.

The slope variance is approximately:

- Unscaled: \( 7.22 \times 10^{-3} \),
- At a scale factor of 10: \( 7.22 \times 10^{-4} \),
- At a scale factor of 100: \( 7.22 \times 10^{-5} \),
- At a scale factor of 1000: \( 7.22 \times 10^{-6} \).

Similarly, the slope variance decreases by a factor of 10 with each scale increase, maintaining consistency with the intercept variance.

The table demonstrates that as the sample size increases (by scaling the counts), the variances of both the intercept and slope parameters decrease significantly.
Multiplying by 10 causes the variance to be divided by 10.
The results demonstrate that both intercept and slope variances decrease proportionally to the increase in the sample size (scale factor). This observation aligns with the theoretical relationship:
\[
\text{Var}(\hat{\beta}) \propto \frac{1}{n}
\]
where \( n \) is the sample size. Such a reduction in variance highlights the importance of larger datasets in improving the precision of parameter estimates in logistic regression models.
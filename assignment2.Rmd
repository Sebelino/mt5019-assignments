---
title: "Computer Assignment 2"
author:
  - "Ville Sebastian Olsson (19911231-1999)"
  - "Arvind Guruprasad (19990212-6938)"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: true
  pdf_document:
    fig_caption: true
---

```{r, echo=FALSE, results='hide', fig.show='hide'}
source("assignment2.R")
list2env(ass, envir = .GlobalEnv)
knitr::opts_chunk$set(echo = FALSE, comment = "")
```

# Exercise 2:1

## 1. \(\beta\) model

The following parameters in the model can be estimated:

* The intercept \(\beta_0\), which represents the log-odds of having periodontitis when
\(x=0\) (i.e. no floss use).
This parameter can be directly estimated from the data because the proportions of periodontitis cases for \(x=0\) are provided by the table.

* The effect parameter \(\beta_1\), which represents the change in log-odds
of having periodontitis between those who use floss (\(x=1\)) and those who
do not (\(x=0\)). This parameter can also be directly estimated because we have the counts for both groups.

Estimates \(\hat\beta_0\) and \(\hat\beta_1\) for these are given by fitting
a logistic regression model and fetching the coefficients of the model.

Coefficients of the model:
```{r}
coef(model21)
```

### Intercept parameter
The estimate for \(\beta_0\) is \(\hat\beta_0\approx -0.58\).
The negative value suggests that
it is more likely for a non-floss user to not have periodontitis than
to have it.
To interpret \(\hat\beta_0\) further,
we can first exponentiate the log-odds to obtain the odds:

\[\text{odds} = e^{\hat\beta_0}\]
which yields the following value:
```{r}
exp(coef(model21)["(Intercept)"])
```

Meaning, for non-floss users, the odds of having periodontitis are about 0.558.
In other words, for every 100 non-floss users who do not have periodontitis, around 56 are expected to have it.

### Effect parameter
The estimate for \(\beta_1\) is \(\hat\beta_1\approx -0.64\).
The negative value suggests that floss users are less likely to have
have periodontitis.

Exponentiating this coefficient gives the odds ratio for floss users relative to non-users:

\[\theta = e^{\hat\beta_1}\]
which yields:
```{r}
exp(coef(model21)["useYes"])
```

This means that floss users have approximately 52.5% of the odds of having periodontitis compared to non-users, suggesting a protective effect of flossing against periodontitis.

## 2. \(\gamma\) model

The following parameters in the model can be estimated:

* The intercept \(\gamma_0\), which represents the log-odds of using dental floss when
\(y=0\) (i.e. no periodontitis).
This parameter can be directly estimated from the data because the proportions of floss uses for \(y=0\) are provided by the table.

* The effect parameter \(\gamma_1\), which represents the change in log-odds
of being a floss user between those with periodontitis (\(y=1\)) and those without it (\(y=0\)). This parameter can also be directly estimated because we have the counts for both groups.

Estimates \(\hat\gamma_0\) and \(\hat\gamma_1\) for these are given by fitting
a logistic regression model and fetching the coefficients of the model.

Coefficients of the model:
```{r}
coef(model21b)
```

### Intercept parameter

The estimate for \(\gamma_0\) is \(\hat\gamma_0\approx -1.3\).
The negative value suggests that
it is more likely for a healthy individual to not use dental floss than
to do it.
To interpret \(\hat\gamma_0\) further,
we can first exponentiate the log-odds to obtain the odds:

\[\text{odds} = e^{\hat\gamma_0}\]
which yields the following value:
```{r}
exp(coef(model21b)["(Intercept)"])
```

Meaning, for healthy individuals, the odds of using floss are about 0.28.
In other words, for every 100 healthy individuals who do not floss, around 28 are expected to floss.

### Effect parameter

The estimate for \(\gamma_1\) is \(\hat\gamma_1\approx -0.64\).
The negative value indicates that individuals with periodontitis (\(y=1\)) are less likely to use dental floss compared to healthy individuals (\(y=0\)).

Exponentiating this coefficient gives the odds ratio for flossing behavior among individuals with periodontitis compared to healthy individuals.

\[\theta = e^{\hat\beta_1}\]
which yields:
```{r}
exp(coef(model21)["useYes"])
```

This means that the odds of using dental floss for individuals with periodontitis are approximately 53 % of the odds for healthy individuals.
This result suggests a behavioral pattern where individuals with periodontitis are less likely to have used dental floss, possibly because abstaining from flossing increases the risk of periodontitis.

## 3. Model comparison

The estimate for \(\beta_0\) is the log-odds of periodontitis for non-floss users, calculated as follows:

\[
\beta_0 = \operatorname{logit}(P(\text{periodontitis} | x=0))
= \ln \left( \frac{P(\text{periodontitis} | x=0)}{1-P(\text{periodontitis} | x=0)}\right)
= \ln \left(\frac{\frac{148}{413}}{1-\frac{148}{413}}\right)
\approx -0.58
\]

The estimate for \( \beta_1 \) is the difference in log-odds of periodontitis between floss users and non-floss users, calculated as follows:

\[
\beta_1 = \ln \left( \frac{\text{odds}_{x=1}(\text{periodontitis})}{\text{odds}_{x=0}(\text{periodontitis})} \right)
= \ln \left( \frac{22/75}{148/265} \right)
\approx -0.64
\]

Numerically:
```{r,echo=TRUE,results='show'}
odds_floss <- 22 / 75
odds_no_floss <- 148 / 265
log_odds_ratio <- log(odds_floss / odds_no_floss)
log_odds_ratio
```

The estimate for \(\gamma_0\) is the log-odds of flossing for healthy individuals,
calculated as follows:

\[
\gamma_0 = \operatorname{logit}(P(\text{floss} | y=0))
= \ln \left( \frac{P(\text{floss} | y=0)}{1-P(\text{floss} | y=0)}\right)
= \ln \left(\frac{\frac{75}{340}}{1-\frac{75}{340}}\right)
\approx -1.3
\]

The estimate for \( \gamma_1 \) is the difference in log-odds of flossing between periodontitis cases and healthy individuals, calculated as follows:

\[
\gamma_1 = \ln \left( \frac{\text{odds}_{y=1}(\text{floss})}{\text{odds}_{y=0}(\text{floss})} \right)
= \ln \left( \frac{22/148}{75/265} \right)
\approx -0.64
\]

Numerical result:
```{r}
odds_periodontitis <- 22 / 148
odds_no_periodontitis <- 75 / 265
log_odds_ratio <- log(odds_periodontitis / odds_no_periodontitis)
log_odds_ratio
```

We observe that \(\gamma_1=\beta_1\approx -0.64\).
This equality holds because both parameters represent
the same log-odds-ratio, but computed differently:
\[e^{\beta_1} = \frac{\text{odds}_{x=1}(\text{periodontitis})}{\text{odds}_{x=0}(\text{periodontitis})} = \frac{22/75}{148/265} = \frac{22/148}{75/265} = \frac{\text{odds}_{y=1}(\text{floss})}{\text{odds}_{y=0}(\text{floss})} = e^{\gamma_1}\]
This simplification relies on the inherent symmetry of odds ratios in a 2Ã—2 table, where the same odds ratio is obtained regardless of whether the roles of the predictor and outcome are reversed.

# Exercise 2:2

## 1. Estimate risk and log-odds

For each dose, the probability of developing a tumor is given by:

\[
\text{Risk} = \frac{\text{Tumor}}{\text{Total}}
\]
where \textit{Total} is the sum of "Tumor" and "No Tumor" counts for each dose level.

\subsection*{Odds of Tumor for Each Dose}
The odds for each dose level can be calculated as:
\[
\text{Log-Odds} = \log \left( \frac{\text{Tumor} / \text{Total}}{1 - (\text{Tumor} / \text{Total})} \right)
\]

We now plot the risk against the log-dose and log-odds against the log-dose:

```{r, align='center'}
plot_risk(log_dose, risk)
```

This plot shows the probability (or risk) of developing a tumor at different log(dose) levels.
As the log(dose) increases, the risk of developing a tumor also increases. This suggests a positive association between dose and tumor risk; higher doses are associated with a greater likelihood of developing a tumor.
The increase in risk appears to follow a smooth trend, becoming especially steep for higher log(dose) values.

```{r, align='center'}
plot_odds(log_dose, log_odds)
```

This plot shows the relationship between log(dose) and the log-odds of developing a tumor.
The log-odds appear to increase in an approximately linear fashion as log(dose) increases. The points follow a roughly straight line, which is a key indicator of a linear relationship.
This linear trend in the log-odds plot suggests that a logistic regression model may be appropriate for modeling the relationship between log(dose) and the probability of developing a tumor. In logistic regression, we assume that the log-odds of the outcome is linearly related to the explanatory variable, and this plot supports that assumption.

Since the log-odds plot shows a roughly linear trend with log(dose), a logistic regression model seems appropriate for these data. The linearity in the log-odds plot indicates that the assumption of logistic regression (a linear relationship between log-odds and the predictor) is likely met. This model would allow us to quantify the effect of dose on the probability of developing a tumor.

## 2. Fit a Logistic Regression Model

We now fit a logistic regression model to the data
to estimate the relationship between does and
the log-odds of developing a tumor.

\(\hat\beta_0\): This represents the log-odds of developing a tumor when the log(dose) is zero. Although it has a mathematical interpretation, it may not have a direct practical meaning in this context, especially if log(dose) = 0 is outside the range of observed values.

\(\hat\beta_1\): The slope tells us how the log-odds of developing a tumor change with each unit increase in log(dose). If \(\hat\beta_1\) is positive, this implies that as log(dose) increases, the log-odds of developing a tumor also increase, indicating a higher probability of tumor development at higher doses.

```{r}
coef(model22)
```

By exponentiating, we obtain the odds:
```{r}
exp(coef(model22)["(Intercept)"])
```
This means that, at a log_dose of zero, the odds of developing a tumor are approximately 1.99.

```{r}
exp(coef(model22)["log_dose"])
```
The positive slope of 0.5204 indicates a strong association between increasing dose levels and a higher likelihood of tumor development.
The odds value of \(1.682\) means that for each one-unit increase in log_dose, the odds of developing a tumor increase by approximately 68.2%. In other words, higher doses are associated with a significantly higher risk of developing a tumor.

## 3. Covariance Matrix and Confidence Interval

Covariance matrix:
```{r}
cov_matrix
```
The diagonal of the covariance matrix
represents the variance of the intercept and the log dose,
respectively.
The off-diagonal elements represent the covariance.
This value indicates the degree to which the intercept and slope vary together.
The off-diagonal values are not zero, meaning that
they are not independent.

The correlation coefficient is given by:
\[\text{Correlation} = \frac{\text{Covariance}}{\text{Standard Error of Intercept} \times \text{Standard Error of Slope}}\]

```{r}
correlation
```
A correlation of approximately 
0.649 indicates a moderate positive correlation between the intercept and slope estimates.
This suggests that the intercept and slope estimates are not completely independent; as one estimate increases, the other tends to increase as well. In practical terms, this moderate correlation is not unusual in logistic regression and does not typically indicate an issue with the model. However, it is useful to be aware of this correlation, especially if we are interpreting the parameters separately.

Confidence interval for the intercept and slope parameter:
```{r}
conf_intervals
```

This interval tells us that we are 95% confident that the true value of the intercept parameter \(\beta_0\) lies between 0.1822 and 1.1918.
In terms of log-odds, the intercept represents the log-odds of developing a tumor when log(dose) = 0.

This interval tells us that we are 95% confident that the true value of the slope parameter \(\beta_1\) lies between 0.3537 and 0.6870.
The slope represents the change in log-odds of developing a tumor for each one-unit increase in log(dose).

Confidence interval for the tumor risk at dose 0.25:
```{r}
probability_ci
```
This interval tells us that we are 95% confident that the true probability of developing a tumor at log(dose) = -1.39 lies between 0.394 (39.4%) and 0.588 (58.8%).
Since this interval is relatively wide, it reflects some uncertainty in the exact probability estimate but still suggests a moderate to high likelihood of tumor development at this dose level.

## 4. Wald Test

We aim to test the null hypothesis:
$$
H_0: \beta_1 = 0
$$
against the alternative hypothesis:
$$
H_1: \beta_1 \neq 0
$$
Numerically computed estimate for \(\beta_1\):
```{r}
beta1
```
Numerically computed standard error for \(\beta_1\):
```{r}
se_beta1
```

The Wald test statistic (squared variant) is given by:
\[t_{W^2} = \frac{(\hat\beta_1-0)^2}{\operatorname{se}(\hat\beta_1)^2}
=\frac{\hat\beta_1^2}{\operatorname{se}(\hat\beta_1)^2}
\]
with realization:
\[t_{W^2} =\frac{0.5204^2}{0.08502^2}\]

where \( \hat{\beta}_1 \) is the estimated coefficient and \( \operatorname{se}(\hat{\beta}_1) \) is its standard error. The test statistic follows a \(\chi^2(1)\) distribution under the null hypothesis.

Numerically computed value for the Wald statistic:
```{r}
wald_statistic
```

\[
p\text{-value} = P(T_{W^2} \geq 37.457)
\]

Numerically computed p-value:
```{r}
p_value24
```

The p-value is much lower than 0.05,
indicating strong evidence against the null hypothesis.
In other words, the slope \(\beta_1\) is significantly different from zero,
meaning that the dose has significant effect on the odds of developing
a tumor.

## 5. Variance and Sample Size Relationship

We aim to investigate the relationship between the sample size and the variances of parameter estimates in logistic regression. Specifically, we scale the sample size by factors of 10, 100, and 1000,
train a new model on each sample size,
compare the square of the standard errors (i.e. variances)
of the intercept and slope parameters.

Numerically computed results:
```{r}
variance_results
```

**Intercept Variance**

- At a scale factor of 10: \( 6.63336 \times 10^{-3} \),
- At a scale factor of 100: \( 6.63336 \times 10^{-4} \),
- At a scale factor of 1000: \( 6.33359 \times 10^{-5} \).

The variance reduces approximately by a factor of 10 for each scale increase.

**Slope Variance**

- At a scale factor of 10: \( 7.22009 \times 10^{-4} \),
- At a scale factor of 100: \( 7.22009 \times 10^{-5} \),
- At a scale factor of 1000: \( 7.22909 \times 10^{-6} \).

Similarly, the slope variance decreases by a factor of 10 with each scale increase, maintaining consistency with the intercept variance.

The table demonstrates that as the sample size increases (by scaling the counts), the variances of both the intercept and slope parameters decrease significantly.
Multiplying by 10 causes the variance to be divided by 10.

**Conclusion**

The results demonstrate that both intercept and slope variances decrease proportionally to the increase in the sample size (scale factor). This observation aligns with the theoretical relationship:
\[
\text{Var}(\hat{\beta}) \propto \frac{1}{n}
\]
where \( n \) is the sample size. Such a reduction in variance highlights the importance of larger datasets in improving the precision of parameter estimates in logistic regression models.